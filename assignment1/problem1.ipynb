{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "f = gzip.open('data/mnist.pkl.gz') \n",
    "mnist = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features = mnist[0][0] #(50000,784)\n",
    "mnist_train_labels = mnist[0][1]   #(50000,1)\n",
    "\n",
    "mnist_valid_features = mnist[1][0] #(10000,784)\n",
    "mnist_valid_labels = mnist[1][1]   #(50000,1)\n",
    "\n",
    "mnist_test_features = mnist[2][0]  #(10000,784)\n",
    "mnist_test_labels = mnist[2][1]    #(50000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataset size for testing, if needed (ex: X_train = mnist_train_features[0:10000])\n",
    "X_train = mnist_train_features[0:50000]\n",
    "y_train = mnist_train_labels[0:50000]\n",
    "\n",
    "X_valid = mnist_valid_features[0:10000]\n",
    "y_valid = mnist_valid_labels[0:10000]\n",
    "\n",
    "X_test = mnist_test_features[0:300]\n",
    "y_test = mnist_test_labels[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,hidden_dims=(1024,2048),weight_mode=None, mode=\"ReLU\"):\n",
    "        self.b1 = np.zeros((hidden_dims[0]))\n",
    "        self.b2 = np.zeros((hidden_dims[1]))\n",
    "        self.b3 = np.zeros((10))\n",
    "        self.learning_rate = 0.0005\n",
    "        self.initialize_weights(hidden_dims,weight_mode)\n",
    "        self.grads = 0\n",
    "        self.act_mode = mode\n",
    "\n",
    "    def initialize_weights(self,hidden_dims,weight_mode):\n",
    "        W1_dim = (784, hidden_dims[0])\n",
    "        W2_dim = (hidden_dims[0], hidden_dims[1])\n",
    "        W3_dim = (hidden_dims[1], 10)\n",
    "        \n",
    "        if weight_mode == \"zero\":\n",
    "            self.W1 = np.zeros(W1_dim)\n",
    "            self.W2 = np.zeros(W2_dim)\n",
    "            self.W3 = np.zeros(W3_dim)\n",
    "            \n",
    "        elif weight_mode == \"normal\":\n",
    "            np.random.seed(23)\n",
    "            self.W1 = np.random.standard_normal(W1_dim)\n",
    "            self.W2 = np.random.standard_normal(W2_dim)\n",
    "            self.W3 = np.random.standard_normal(W3_dim)\n",
    "            \n",
    "        elif weight_mode == \"glorot\":\n",
    "            np.random.seed(23)\n",
    "            \n",
    "            d1 = math.sqrt(6.0 / (W1_dim[0] + W1_dim[1]))\n",
    "            d2 = math.sqrt(6.0 / (W2_dim[0] + W2_dim[1]))\n",
    "            d3 = math.sqrt(6.0 / (W3_dim[0] + W3_dim[1]))\n",
    "            \n",
    "            self.W1 = np.random.uniform(-d1, d1, W1_dim)\n",
    "            self.W2 = np.random.uniform(-d2, d2, W2_dim)\n",
    "            self.W3 = np.random.uniform(-d3, d3, W3_dim)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Unsupported weight_mode value\")\n",
    "\n",
    "    def forward(self,data):\n",
    "        cache = {}\n",
    "        \n",
    "        cache[\"h0\"] = np.copy(data)\n",
    "        cache[\"a1\"] = np.dot(cache[\"h0\"], self.W1) + self.b1\n",
    "        cache[\"h1\"] = self.activation(cache[\"a1\"], self.act_mode)\n",
    "        cache[\"a2\"] = np.dot(cache[\"h1\"], self.W2) + self.b2\n",
    "        cache[\"h2\"] = self.activation(cache[\"a2\"], self.act_mode)\n",
    "        cache[\"a3\"] = np.dot(cache[\"h2\"], self.W3) + self.b3\n",
    "        cache[\"h3\"] = self.softmax(cache[\"a3\"])\n",
    "        \n",
    "        return cache\n",
    "\n",
    "    def activation(self,data, mode=\"ReLU\"):\n",
    "        if(mode == \"ReLU\"):\n",
    "            return self.ReLU(data)\n",
    "        elif(mode == \"sigmoid\"):\n",
    "            return self.sigmoid(data)\n",
    "        else:\n",
    "            raise Exception(\"Not an activation function\")\n",
    "    \n",
    "    def activation_derivative(self,data, mode=\"ReLU\"): # RELU derivative\n",
    "        if(mode == \"ReLU\"):\n",
    "            return self.ReLU_derivative(data)\n",
    "        elif(mode == \"sigmoid\"):\n",
    "            return self.sigmoid_derivative(data)\n",
    "        else:\n",
    "            raise Exception(\"Not an activation function\")\n",
    "    \n",
    "    def sigmoid(self, data):\n",
    "        return 1/(1+np.exp(-data))\n",
    "    \n",
    "    def sigmoid_derivative(self,data):\n",
    "        return self.sigmoid(data)*(1-self.sigmoid(data))\n",
    "    \n",
    "    def ReLU(self,data): # RELU\n",
    "        return np.where(data > 0, data, 0.0)\n",
    "    \n",
    "    def ReLU_derivative(self,data):\n",
    "        return np.where(data > 0, 1.0, 0.0)\n",
    "\n",
    "    def loss(self, data, labels, eps = 1e-15): # Cross Entropy loss\n",
    "        cache = self.forward(data)\n",
    "        preds = np.clip(cache[\"h3\"], eps, 1 - eps)\n",
    "        preds = preds / np.sum(preds, axis=1)[:, np.newaxis]\n",
    "        one_hot_truth = np.eye(10)[labels]\n",
    "        log_loss = -np.diag(np.matmul(one_hot_truth,np.log(preds).T))\n",
    "        return  np.average(log_loss)\n",
    "\n",
    "    def softmax(self,data): # Numerically stable softmax\n",
    "        result = np.exp(data - np.max(data, axis=1, keepdims=True))\n",
    "        result = result / np.sum(result, axis=1, keepdims=True)\n",
    "        return result\n",
    "\n",
    "    def backward(self,cache,labels):\n",
    "        grads = {}\n",
    "\n",
    "        grads[\"a3\"] = cache[\"h3\"] - np.eye(10)[labels]\n",
    "        grads[\"W3\"] = np.dot(cache[\"h2\"].T, grads[\"a3\"])\n",
    "        grads[\"b3\"] = np.sum(grads[\"a3\"], axis=0)\n",
    "        \n",
    "        grads[\"h2\"] = np.dot(grads[\"a3\"], self.W3.T)\n",
    "        grads[\"a2\"] = grads[\"h2\"] * self.activation_derivative(cache[\"a2\"], self.act_mode)\n",
    "        grads[\"W2\"] = np.dot(cache[\"h1\"].T, grads[\"a2\"])\n",
    "        grads[\"b2\"] = np.sum(grads[\"a2\"], axis=0)\n",
    "        \n",
    "        grads[\"h1\"] = np.dot(grads[\"a2\"], self.W2.T)\n",
    "        grads[\"a1\"] = grads[\"h1\"] * self.activation_derivative(cache[\"a1\"], self.act_mode)\n",
    "        grads[\"W1\"] = np.dot(cache[\"h0\"].T, grads[\"a1\"])\n",
    "        grads[\"b1\"] = np.sum(grads[\"a1\"], axis=0)\n",
    "        \n",
    "        self.grads = grads\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def update(self,grads):\n",
    "        self.b3 -= (self.learning_rate * grads[\"b3\"])\n",
    "        self.W3 -= (self.learning_rate * grads[\"W3\"])\n",
    "        self.b2 -= (self.learning_rate * grads[\"b2\"])\n",
    "        self.W2 -= (self.learning_rate * grads[\"W2\"])\n",
    "        self.b1 -= (self.learning_rate * grads[\"b1\"])\n",
    "        self.W1 -= (self.learning_rate * grads[\"W1\"])\n",
    "\n",
    "    def train(self, train_data, train_labels, valid_data, valid_labels, mini_batch_size = 0, epochs = 10, learning_rate = 0.0005, lr_decay = 1, lr_decay_intervals = 0):\n",
    "        self.learning_rate = learning_rate\n",
    "        train_info = {}\n",
    "        train_info[\"train_loss\"] = []\n",
    "        train_info[\"train_accuracy\"] = []\n",
    "        train_info[\"valid_loss\"] = []\n",
    "        train_info[\"valid_accuracy\"] = []\n",
    "        decay_flag = True\n",
    "        \n",
    "        data_mini = []\n",
    "        label_mini = []\n",
    "        \n",
    "        if (mini_batch_size==0):\n",
    "            mini_batch_size = np.shape(train_data)[0]\n",
    "            \n",
    "        if (lr_decay==1):\n",
    "            decay_flag = False\n",
    "            \n",
    "        if (lr_decay>1):\n",
    "            raise ValueError(\"lr_decay has to be equal or under to 1\")\n",
    "            \n",
    "        if (decay_flag==True):\n",
    "            if lr_decay_intervals!=0:\n",
    "                lr_decay_intervals = lr_decay_intervals\n",
    "            else:\n",
    "                lr_decay_intervals = [0.2,0.6]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, np.shape(train_data)[0], mini_batch_size):\n",
    "                \n",
    "                #Mini-batch training\n",
    "                data_mini = train_data[i:i + mini_batch_size]\n",
    "                labels_mini = train_labels[i:i + mini_batch_size]\n",
    "                \n",
    "                #forward and backward propagation\n",
    "                cache = self.forward(data_mini)\n",
    "                grads = self.backward(cache,labels_mini)\n",
    "                self.update(grads)\n",
    "                \n",
    "            #Measuring loss and accuracy\n",
    "            predictions = np.argmax(cache[\"h3\"], axis=1)\n",
    "            train_accuracy = np.mean(predictions == labels_mini)\n",
    "            train_loss = self.loss(data_mini, labels_mini)\n",
    "            \n",
    "            valid_cache = self.forward(valid_data)\n",
    "            valid_pred = np.argmax(valid_cache[\"h3\"], axis=1)\n",
    "            valid_accuracy = np.mean(valid_pred == valid_labels)\n",
    "            valid_loss = self.loss(valid_data, valid_labels)\n",
    "            \n",
    "            if(decay_flag):\n",
    "                if (epoch in np.dot(lr_decay_intervals,epochs)):\n",
    "                    learning_rate = learning_rate * lr_decay\n",
    "\n",
    "            \n",
    "            train_info[\"train_loss\"].append(train_loss)\n",
    "            train_info[\"train_accuracy\"].append(train_accuracy)   \n",
    "            train_info[\"valid_loss\"].append(valid_loss)\n",
    "            train_info[\"valid_accuracy\"].append(valid_accuracy)\n",
    "            print(f\"Training epoch \", epoch + 1, \"\\t\", \"Accuracy: {0:.4f}\".format(train_accuracy), \"\\t\", \"Loss: {0:.4f}\".format(train_loss))\n",
    "            print(f\"Validation \", \"\\t\", \"\\t\", \"Accuracy: {0:.4f}\".format(valid_accuracy), \"\\t\", \"Loss: {0:.4f}\".format(valid_loss), '\\n')\n",
    "        \n",
    "        return train_info\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        cache = self.forward(data)\n",
    "        predictions = cache[\"h3\"].argmax(1).astype(int)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero\n",
      "Training epoch  1 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "\n",
      "normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:109: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/pun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in greater\n",
      "/home/pun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  1 \t Accuracy: 0.0900 \t Loss: 2.3028\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 0.0900 \t Loss: 2.3025\n",
      "Validation  \t \t Accuracy: 0.1064 \t Loss: 2.3018 \n",
      "\n",
      "\n",
      "glorot\n",
      "Training epoch  1 \t Accuracy: 0.9400 \t Loss: 0.1378\n",
      "Validation  \t \t Accuracy: 0.9428 \t Loss: 0.2076 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9700 \t Loss: 0.0746\n",
      "Validation  \t \t Accuracy: 0.9592 \t Loss: 0.1450 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 0.9800 \t Loss: 0.0492\n",
      "Validation  \t \t Accuracy: 0.9670 \t Loss: 0.1169 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 0.9800 \t Loss: 0.0352\n",
      "Validation  \t \t Accuracy: 0.9714 \t Loss: 0.1017 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 0.9800 \t Loss: 0.0272\n",
      "Validation  \t \t Accuracy: 0.9739 \t Loss: 0.0917 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 0.9900 \t Loss: 0.0217\n",
      "Validation  \t \t Accuracy: 0.9738 \t Loss: 0.0847 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 0.9900 \t Loss: 0.0182\n",
      "Validation  \t \t Accuracy: 0.9754 \t Loss: 0.0798 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 0.9900 \t Loss: 0.0153\n",
      "Validation  \t \t Accuracy: 0.9756 \t Loss: 0.0764 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 0.9900 \t Loss: 0.0130\n",
      "Validation  \t \t Accuracy: 0.9765 \t Loss: 0.0742 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 0.9900 \t Loss: 0.0110\n",
      "Validation  \t \t Accuracy: 0.9777 \t Loss: 0.0726 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for weight_mode in [\"zero\", \"normal\", \"glorot\"]:\n",
    "    print(weight_mode)\n",
    "    nn = NN(hidden_dims=(800,300),weight_mode=weight_mode, mode=\"ReLU\")\n",
    "    train_info = nn.train(X_train, y_train, X_valid, y_valid, epochs = 10, mini_batch_size=100, learning_rate = 0.001, lr_decay = 0.5)\n",
    "    plt.plot(np.arange(1,11,1), train_info[\"train_loss\"], label=weight_mode)\n",
    "    print()\n",
    "\n",
    "plt.title(\"Average loss by epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0321\n",
      "Validation  \t \t Accuracy: 0.9637 \t Loss: 0.1215 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0047\n",
      "Validation  \t \t Accuracy: 0.9645 \t Loss: 0.1145 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 0.9375 \t Loss: 0.0020\n",
      "Validation  \t \t Accuracy: 0.9679 \t Loss: 0.1069 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0040\n",
      "Validation  \t \t Accuracy: 0.9765 \t Loss: 0.0837 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0038\n",
      "Validation  \t \t Accuracy: 0.9767 \t Loss: 0.0859 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9789 \t Loss: 0.0807 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0007\n",
      "Validation  \t \t Accuracy: 0.9716 \t Loss: 0.1111 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.0854 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9800 \t Loss: 0.0878 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9805 \t Loss: 0.0874 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9751 \t Loss: 0.1116 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9810 \t Loss: 0.0849 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0003\n",
      "Validation  \t \t Accuracy: 0.9806 \t Loss: 0.0908 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0010\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.1014 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9810 \t Loss: 0.0940 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9809 \t Loss: 0.0974 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0006\n",
      "Validation  \t \t Accuracy: 0.9828 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0005\n",
      "Validation  \t \t Accuracy: 0.9843 \t Loss: 0.0877 \n",
      "\n",
      "Training epoch  19 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9833 \t Loss: 0.0949 \n",
      "\n",
      "Training epoch  20 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9842 \t Loss: 0.0910 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0321\n",
      "Validation  \t \t Accuracy: 0.9637 \t Loss: 0.1215 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0047\n",
      "Validation  \t \t Accuracy: 0.9645 \t Loss: 0.1145 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 0.9375 \t Loss: 0.0020\n",
      "Validation  \t \t Accuracy: 0.9679 \t Loss: 0.1069 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0040\n",
      "Validation  \t \t Accuracy: 0.9765 \t Loss: 0.0837 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0038\n",
      "Validation  \t \t Accuracy: 0.9767 \t Loss: 0.0859 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9789 \t Loss: 0.0807 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0007\n",
      "Validation  \t \t Accuracy: 0.9716 \t Loss: 0.1111 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.0854 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9800 \t Loss: 0.0878 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9805 \t Loss: 0.0874 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9751 \t Loss: 0.1116 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9810 \t Loss: 0.0849 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0003\n",
      "Validation  \t \t Accuracy: 0.9806 \t Loss: 0.0908 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0010\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.1014 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9810 \t Loss: 0.0940 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9809 \t Loss: 0.0974 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0006\n",
      "Validation  \t \t Accuracy: 0.9828 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0005\n",
      "Validation  \t \t Accuracy: 0.9843 \t Loss: 0.0877 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0321\n",
      "Validation  \t \t Accuracy: 0.9637 \t Loss: 0.1215 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0047\n",
      "Validation  \t \t Accuracy: 0.9645 \t Loss: 0.1145 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 0.9375 \t Loss: 0.0020\n",
      "Validation  \t \t Accuracy: 0.9679 \t Loss: 0.1069 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0040\n",
      "Validation  \t \t Accuracy: 0.9765 \t Loss: 0.0837 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0038\n",
      "Validation  \t \t Accuracy: 0.9767 \t Loss: 0.0859 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9789 \t Loss: 0.0807 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0007\n",
      "Validation  \t \t Accuracy: 0.9716 \t Loss: 0.1111 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.0854 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9800 \t Loss: 0.0878 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9805 \t Loss: 0.0874 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9751 \t Loss: 0.1116 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9810 \t Loss: 0.0849 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0003\n",
      "Validation  \t \t Accuracy: 0.9806 \t Loss: 0.0908 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0010\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.1014 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9810 \t Loss: 0.0940 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9809 \t Loss: 0.0974 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0006\n",
      "Validation  \t \t Accuracy: 0.9828 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0005\n",
      "Validation  \t \t Accuracy: 0.9843 \t Loss: 0.0877 \n",
      "\n",
      "Training epoch  19 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9833 \t Loss: 0.0949 \n",
      "\n",
      "Training epoch  20 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9842 \t Loss: 0.0910 \n",
      "\n",
      "Training epoch  21 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9845 \t Loss: 0.0921 \n",
      "\n",
      "Training epoch  22 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9845 \t Loss: 0.0930 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8125 \t Loss: 0.0285\n",
      "Validation  \t \t Accuracy: 0.9623 \t Loss: 0.1279 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 1.0000 \t Loss: 0.0108\n",
      "Validation  \t \t Accuracy: 0.9646 \t Loss: 0.1163 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0057\n",
      "Validation  \t \t Accuracy: 0.9703 \t Loss: 0.0955 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 0.9375 \t Loss: 0.0017\n",
      "Validation  \t \t Accuracy: 0.9760 \t Loss: 0.0861 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0032\n",
      "Validation  \t \t Accuracy: 0.9784 \t Loss: 0.0798 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0023\n",
      "Validation  \t \t Accuracy: 0.9754 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0031\n",
      "Validation  \t \t Accuracy: 0.9762 \t Loss: 0.1011 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9809 \t Loss: 0.0844 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9781 \t Loss: 0.1037 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0008\n",
      "Validation  \t \t Accuracy: 0.9786 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9812 \t Loss: 0.0928 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0007\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0006\n",
      "Validation  \t \t Accuracy: 0.9817 \t Loss: 0.0946 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9829 \t Loss: 0.0908 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9830 \t Loss: 0.0910 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9839 \t Loss: 0.0911 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9837 \t Loss: 0.0919 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0925 \n",
      "\n",
      "Training epoch  19 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9842 \t Loss: 0.0932 \n",
      "\n",
      "Training epoch  20 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9843 \t Loss: 0.0937 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  1 \t Accuracy: 0.8125 \t Loss: 0.0285\n",
      "Validation  \t \t Accuracy: 0.9623 \t Loss: 0.1279 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 1.0000 \t Loss: 0.0108\n",
      "Validation  \t \t Accuracy: 0.9646 \t Loss: 0.1163 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0057\n",
      "Validation  \t \t Accuracy: 0.9703 \t Loss: 0.0955 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 0.9375 \t Loss: 0.0017\n",
      "Validation  \t \t Accuracy: 0.9760 \t Loss: 0.0861 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0032\n",
      "Validation  \t \t Accuracy: 0.9784 \t Loss: 0.0798 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0023\n",
      "Validation  \t \t Accuracy: 0.9754 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0031\n",
      "Validation  \t \t Accuracy: 0.9762 \t Loss: 0.1011 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9809 \t Loss: 0.0844 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9781 \t Loss: 0.1037 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0008\n",
      "Validation  \t \t Accuracy: 0.9786 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9812 \t Loss: 0.0928 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0007\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0006\n",
      "Validation  \t \t Accuracy: 0.9817 \t Loss: 0.0946 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9829 \t Loss: 0.0908 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9830 \t Loss: 0.0910 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9839 \t Loss: 0.0911 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9837 \t Loss: 0.0919 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0925 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8125 \t Loss: 0.0285\n",
      "Validation  \t \t Accuracy: 0.9623 \t Loss: 0.1279 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 1.0000 \t Loss: 0.0108\n",
      "Validation  \t \t Accuracy: 0.9646 \t Loss: 0.1163 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0057\n",
      "Validation  \t \t Accuracy: 0.9703 \t Loss: 0.0955 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 0.9375 \t Loss: 0.0017\n",
      "Validation  \t \t Accuracy: 0.9760 \t Loss: 0.0861 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0032\n",
      "Validation  \t \t Accuracy: 0.9784 \t Loss: 0.0798 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0023\n",
      "Validation  \t \t Accuracy: 0.9754 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0031\n",
      "Validation  \t \t Accuracy: 0.9762 \t Loss: 0.1011 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9809 \t Loss: 0.0844 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9781 \t Loss: 0.1037 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0008\n",
      "Validation  \t \t Accuracy: 0.9786 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9812 \t Loss: 0.0928 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0007\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0006\n",
      "Validation  \t \t Accuracy: 0.9817 \t Loss: 0.0946 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9829 \t Loss: 0.0908 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9830 \t Loss: 0.0910 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9839 \t Loss: 0.0911 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9837 \t Loss: 0.0919 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0925 \n",
      "\n",
      "Training epoch  19 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9842 \t Loss: 0.0932 \n",
      "\n",
      "Training epoch  20 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9843 \t Loss: 0.0937 \n",
      "\n",
      "Training epoch  21 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9843 \t Loss: 0.0942 \n",
      "\n",
      "Training epoch  22 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9844 \t Loss: 0.0947 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0254\n",
      "Validation  \t \t Accuracy: 0.9587 \t Loss: 0.1352 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0084\n",
      "Validation  \t \t Accuracy: 0.9692 \t Loss: 0.1010 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0030\n",
      "Validation  \t \t Accuracy: 0.9733 \t Loss: 0.0875 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9772 \t Loss: 0.0827 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0026\n",
      "Validation  \t \t Accuracy: 0.9783 \t Loss: 0.0839 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.0831 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0011\n",
      "Validation  \t \t Accuracy: 0.9788 \t Loss: 0.0942 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9779 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0835 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9787 \t Loss: 0.0922 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.1009 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9808 \t Loss: 0.0922 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.1015 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0003\n",
      "Validation  \t \t Accuracy: 0.9826 \t Loss: 0.0881 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9803 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9822 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9835 \t Loss: 0.0904 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9837 \t Loss: 0.0861 \n",
      "\n",
      "Training epoch  19 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0875 \n",
      "\n",
      "Training epoch  20 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0883 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0254\n",
      "Validation  \t \t Accuracy: 0.9587 \t Loss: 0.1352 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0084\n",
      "Validation  \t \t Accuracy: 0.9692 \t Loss: 0.1010 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0030\n",
      "Validation  \t \t Accuracy: 0.9733 \t Loss: 0.0875 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9772 \t Loss: 0.0827 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0026\n",
      "Validation  \t \t Accuracy: 0.9783 \t Loss: 0.0839 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.0831 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0011\n",
      "Validation  \t \t Accuracy: 0.9788 \t Loss: 0.0942 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9779 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0835 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9787 \t Loss: 0.0922 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.1009 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9808 \t Loss: 0.0922 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.1015 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0003\n",
      "Validation  \t \t Accuracy: 0.9826 \t Loss: 0.0881 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9803 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9822 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9835 \t Loss: 0.0904 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9837 \t Loss: 0.0861 \n",
      "\n",
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0254\n",
      "Validation  \t \t Accuracy: 0.9587 \t Loss: 0.1352 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0084\n",
      "Validation  \t \t Accuracy: 0.9692 \t Loss: 0.1010 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0030\n",
      "Validation  \t \t Accuracy: 0.9733 \t Loss: 0.0875 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9772 \t Loss: 0.0827 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0026\n",
      "Validation  \t \t Accuracy: 0.9783 \t Loss: 0.0839 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.0831 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0011\n",
      "Validation  \t \t Accuracy: 0.9788 \t Loss: 0.0942 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9779 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0835 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9787 \t Loss: 0.0922 \n",
      "\n",
      "Training epoch  11 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.1009 \n",
      "\n",
      "Training epoch  12 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9808 \t Loss: 0.0922 \n",
      "\n",
      "Training epoch  13 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9801 \t Loss: 0.1015 \n",
      "\n",
      "Training epoch  14 \t Accuracy: 1.0000 \t Loss: 0.0003\n",
      "Validation  \t \t Accuracy: 0.9826 \t Loss: 0.0881 \n",
      "\n",
      "Training epoch  15 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9803 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  16 \t Accuracy: 1.0000 \t Loss: 0.0002\n",
      "Validation  \t \t Accuracy: 0.9822 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  17 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9835 \t Loss: 0.0904 \n",
      "\n",
      "Training epoch  18 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9837 \t Loss: 0.0861 \n",
      "\n",
      "Training epoch  19 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0875 \n",
      "\n",
      "Training epoch  20 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0883 \n",
      "\n",
      "Training epoch  21 \t Accuracy: 1.0000 \t Loss: 0.0001\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0891 \n",
      "\n",
      "Training epoch  22 \t Accuracy: 1.0000 \t Loss: 0.0000\n",
      "Validation  \t \t Accuracy: 0.9841 \t Loss: 0.0899 \n",
      "\n",
      "0.9845 {'lr': 0.0085, 'hd': (625, 625), 'mb': 64, 'epoch': 22}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "\n",
    "# Fix this to change the way we see the results; h.p. ahve to be determined through curve analysis\n",
    "\n",
    "lr = [0.0085, 0.008, 0.0075]\n",
    "hiddens_dims = [(625,625)]\n",
    "mini_batch = [64]\n",
    "epoch = (20,18,22)\n",
    "\n",
    "\n",
    "# Mode = ReLU\n",
    "best = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr_ in lr:\n",
    "    for hd in hiddens_dims:\n",
    "        for mb in mini_batch:\n",
    "            for epoch_ in epoch:\n",
    "                #print(\"Lr : {}, hd : {}, mb : {}\".format(lr_, hd, mb))\n",
    "                model = NN(hidden_dims = hd, weight_mode = \"glorot\", mode = \"ReLU\")\n",
    "                train_log = model.train(X_train, y_train, X_valid, y_valid, epochs = epoch_,\n",
    "                                        mini_batch_size=mb, learning_rate = lr_, lr_decay = 0.8,\n",
    "                                        lr_decay_intervals = [0.2, 0.5, 0.8])\n",
    "                acc = model.test(X_valid, y_valid)\n",
    "                if(acc>best):\n",
    "                    best = acc\n",
    "                    best_params[\"lr\"] = lr_\n",
    "                    best_params[\"hd\"] = hd\n",
    "                    best_params[\"mb\"] = mb\n",
    "                    best_params[\"epoch\"]=epoch_\n",
    "                              \n",
    "print(best, best_params)\n",
    "\n",
    "# # Mode = sigmoid\n",
    "# best_s = 0\n",
    "# best_params_s = {}\n",
    "\n",
    "# for lr_ in lr:\n",
    "#     for hd in hiddens_dims:\n",
    "#         for mb in mini_batch:\n",
    "#             for epoch_ in epoch:\n",
    "#                 #print(\"Lr : {}, hd : {}, mb : {}\".format(lr_, hd, mb))\n",
    "#                 model = NN(hidden_dims = hd, weight_mode = \"glorot\", mode = \"sigmoid\")\n",
    "#                 train_log = model.train(X_train, y_train, epochs = epoch_, mini_batch_size=mb, learning_rate = lr_)\n",
    "#                 acc = model.test(X_valid, y_valid)\n",
    "#                 if(acc>best_s):\n",
    "#                     best_s = acc\n",
    "#                     best_params_s[\"lr\"] = lr_\n",
    "#                     best_params_s[\"hd\"] = hd\n",
    "#                     best_params_s[\"mb\"] = mb\n",
    "#                     best_params_s[\"epoch\"]=epoch_\n",
    "                              \n",
    "# print(best_s, best_params_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96.3% lr = 0.008, hd = 625,625, mb = 64, epoch = 40\n",
    "\n",
    "96.8% lr = 0.008, hd = 625,625, mb = 64, epoch = 20\n",
    "\n",
    "98.5% lr = 0.0075, hd = 625, 625, mb = 64, epoch = 22\n",
    "\n",
    "\n",
    "^^^^^^^^Tous en overfitting^^^^^^^^\n",
    "\n",
    "98.5% lr = 0.0075, hd = 625, 625, mb = 64, epoch = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Gradients Using Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  1 \t Accuracy: 0.8750 \t Loss: 0.0254\n",
      "Validation  \t \t Accuracy: 0.9587 \t Loss: 0.1352 \n",
      "\n",
      "Training epoch  2 \t Accuracy: 0.9375 \t Loss: 0.0084\n",
      "Validation  \t \t Accuracy: 0.9692 \t Loss: 0.1010 \n",
      "\n",
      "Training epoch  3 \t Accuracy: 1.0000 \t Loss: 0.0030\n",
      "Validation  \t \t Accuracy: 0.9733 \t Loss: 0.0875 \n",
      "\n",
      "Training epoch  4 \t Accuracy: 1.0000 \t Loss: 0.0027\n",
      "Validation  \t \t Accuracy: 0.9772 \t Loss: 0.0827 \n",
      "\n",
      "Training epoch  5 \t Accuracy: 1.0000 \t Loss: 0.0026\n",
      "Validation  \t \t Accuracy: 0.9783 \t Loss: 0.0839 \n",
      "\n",
      "Training epoch  6 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9791 \t Loss: 0.0831 \n",
      "\n",
      "Training epoch  7 \t Accuracy: 1.0000 \t Loss: 0.0011\n",
      "Validation  \t \t Accuracy: 0.9788 \t Loss: 0.0942 \n",
      "\n",
      "Training epoch  8 \t Accuracy: 1.0000 \t Loss: 0.0012\n",
      "Validation  \t \t Accuracy: 0.9779 \t Loss: 0.0893 \n",
      "\n",
      "Training epoch  9 \t Accuracy: 1.0000 \t Loss: 0.0013\n",
      "Validation  \t \t Accuracy: 0.9811 \t Loss: 0.0835 \n",
      "\n",
      "Training epoch  10 \t Accuracy: 1.0000 \t Loss: 0.0009\n",
      "Validation  \t \t Accuracy: 0.9787 \t Loss: 0.0922 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code taken from Marc-André's homework#3 of IFT6390 class.\n",
    "lr = 0.0075\n",
    "hd = (625,625)\n",
    "mb = 64\n",
    "epoch = 10\n",
    "eps = (1/10, 1/50, 1/100, 1/500, 1/1000)\n",
    "\n",
    "model_finite_diff = NN(hidden_dims=hd, weight_mode=\"glorot\", mode=\"ReLU\")\n",
    "log = model_finite_diff.train(X_train, y_train, X_valid, y_valid, epochs = epoch,\n",
    "                                        mini_batch_size=mb, learning_rate = lr, lr_decay = 0.8,\n",
    "                                        lr_decay_intervals = [0.2, 0.5, 0.8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6194ffbc88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(np.arange(1,11,1), log[\"train_loss\"], label = \"Train loss\")\n",
    "plt.plot(np.arange(1,11,1), log[\"valid_loss\"], label = \"Valid loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(np.arange(1,11,1), log[\"train_accuracy\"], label = \"Train accuracy\")\n",
    "plt.plot(np.arange(1,11,1), log[\"valid_accuracy\"], label = \"Valid accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5619720929012424\n",
      "0.005513879069845523\n",
      "-0.4380279070987576\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  2.5117005428735896e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.3953054262345757\n",
      "0.005513879069845523\n",
      "-0.2713612404320909\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  2.5117005428735896e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.3119720929012424\n",
      "0.005513879069845523\n",
      "-0.1880279070987576\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  2.5117005428735896e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.2619720929012424\n",
      "0.005513879069845523\n",
      "-0.13802790709875762\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  2.5117005428735896e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.4613983927872738\n",
      "0.005513879069845523\n",
      "-0.5386016072127262\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.2947317261206071\n",
      "0.005513879069845523\n",
      "-0.3719349405460595\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.2113983927872738\n",
      "0.005513879069845523\n",
      "-0.2886016072127262\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.1613983927872738\n",
      "0.005513879069845523\n",
      "-0.23860160721272622\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.45563627382423566\n",
      "0.005513879069845523\n",
      "-0.5443637261757643\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.288969607157569\n",
      "0.005513879069845523\n",
      "-0.37769705950909765\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.20563627382423566\n",
      "0.005513879069845523\n",
      "-0.29436372617576434\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.15563627382423567\n",
      "0.005513879069845523\n",
      "-0.24436372617576435\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.5542521061019543\n",
      "0.005513879069845523\n",
      "-0.4457478938980457\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.3875854394352876\n",
      "0.005513879069845523\n",
      "-0.279081227231379\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.3042521061019543\n",
      "0.005513879069845523\n",
      "-0.1957478938980457\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.2542521061019543\n",
      "0.005513879069845523\n",
      "-0.1457478938980457\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.3756077518248688\n",
      "0.005513879069845523\n",
      "-0.6243922481751312\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.20894108515820214\n",
      "0.005513879069845523\n",
      "-0.4577255815084645\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.12560775182486883\n",
      "0.005513879069845523\n",
      "-0.37439224817513117\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.07560775182486884\n",
      "0.005513879069845523\n",
      "-0.3243922481751312\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.4291931921676001\n",
      "0.005513879069845523\n",
      "-0.5708068078323999\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.2625265255009334\n",
      "0.005513879069845523\n",
      "-0.4041401411657332\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.1791931921676001\n",
      "0.005513879069845523\n",
      "-0.3208068078323999\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.1291931921676001\n",
      "0.005513879069845523\n",
      "-0.2708068078323999\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.4592269888363272\n",
      "0.005513879069845523\n",
      "-0.5407730111636728\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  1.3652673269035482e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.2925603221696605\n",
      "0.005513879069845523\n",
      "-0.37410634449700614\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  1.3652673269035482e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.20922698883632718\n",
      "0.005513879069845523\n",
      "-0.2907730111636728\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  1.3652673269035482e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.1592269888363272\n",
      "0.005513879069845523\n",
      "-0.24077301116367283\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  1.3652673269035482e-06\n",
      "Ratio :  0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.5486129449826592\n",
      "0.005513879069845523\n",
      "-0.4513870550173408\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -6.16840390919904e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.38194627831599254\n",
      "0.005513879069845523\n",
      "-0.2847203883506741\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -6.16840390919904e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.2986129449826592\n",
      "0.005513879069845523\n",
      "-0.20138705501734078\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -6.16840390919904e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.24861294498265923\n",
      "0.005513879069845523\n",
      "-0.1513870550173408\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -6.16840390919904e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.4430492210593302\n",
      "0.005513879069845523\n",
      "-0.5569507789406698\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.2763825543926635\n",
      "0.005513879069845523\n",
      "-0.3902841122740031\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.1930492210593302\n",
      "0.005513879069845523\n",
      "-0.3069507789406698\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.1430492210593302\n",
      "0.005513879069845523\n",
      "-0.2569507789406698\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.5\n",
      "0.5356836714986732\n",
      "0.005513879069845523\n",
      "-0.46431632850132676\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -2.3433136162645016e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.3333333333333333\n",
      "0.36901700483200656\n",
      "0.005513879069845523\n",
      "-0.2976496618346601\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -2.3433136162645016e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.25\n",
      "0.28568367149867324\n",
      "0.005513879069845523\n",
      "-0.21431632850132676\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -2.3433136162645016e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n",
      "0.2\n",
      "0.23568367149867325\n",
      "0.005513879069845523\n",
      "-0.16431632850132677\n",
      "0.005513879069845523\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  -2.3433136162645016e-06\n",
      "Ratio :  -0.0\n",
      "----------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def finite_diff_gradient_check(model, sample_X, sample_Y, weights_idx = 10, eps = (1/100)):\n",
    "    \"\"\"\n",
    "    -----\n",
    "    Parameters: \n",
    "        model: neural net with initialized weights\n",
    "        weights_idx: first weights to evaluate theta = 1:weights_idx\n",
    "        eps: epsilon values to try for finite difference\n",
    "        sample_X : data point\n",
    "        sample_Y : data point label\n",
    "    \"\"\"\n",
    "    for i in range(weights_idx):\n",
    "        for eps_ in eps:\n",
    "            print(eps_)\n",
    "            model.W1[300,30*i] += eps_\n",
    "            print(model.W1[300,30*i])\n",
    "            loss_eps_plus = model.loss(sample_X, sample_Y)\n",
    "            print(loss_eps_plus)\n",
    "            model.W1[300,30*i] -= 2*eps_\n",
    "            print(model.W1[300,30*i])\n",
    "            loss_eps_minus = model.loss(sample_X, sample_Y)\n",
    "            model.W1[300,30*i] += eps_\n",
    "            print(loss_eps_minus)\n",
    "            finite_diff = (loss_eps_plus - loss_eps_minus)/(2*eps_)\n",
    "            print(\"Finite Gradient : \",finite_diff)\n",
    "            print(\"Backprop gradient : \", model.grads[\"W1\"][300,30*i])\n",
    "            print(\"Ratio : \", finite_diff / model.grads[\"W1\"][300,30*i])\n",
    "            print(\"-----------------------------------\", '\\n')\n",
    "\n",
    "            \n",
    "\n",
    "finite_diff_gradient_check(model_finite_diff, (X_train[0, np.newaxis]), y_train[0], eps=(1/2, 1/3, 1/4, 1/5))\n",
    "\n",
    "#Il me semble que ce résultat est normal, les premiers paramètres de W1 pointent vers des pixels TOUJOURS vide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_Y = to_categorical(Y, num_classes=self.m).reshape((1, self.m))\n",
    "self.bprop(X,onehot_Y)\n",
    "\n",
    "loss_fprop = self.loss\n",
    "\n",
    "eta = 10**(-5)\n",
    "\n",
    "finite_grad_W1 = self.check_onebyone(X, Y, eta, loss_fprop, self.W1, (self.dh, self.d))\n",
    "ratio_W1 = np.nanmean(finite_grad_W1/self.grad_W1)\n",
    "\n",
    "finite_grad_W2 = self.check_onebyone(X, Y, eta, loss_fprop, self.W2, (self.m, self.dh))\n",
    "ratio_W2 = np.nanmean(finite_grad_W2/self.grad_W2)\n",
    "\n",
    "finite_grad_b1 = self.check_onebyone(X, Y, eta, loss_fprop, self.b1, (1, self.dh))\n",
    "ratio_b1 = np.nanmean(finite_grad_b1/self.grad_b1.reshape((1, self.dh)))\n",
    "\n",
    "finite_grad_b2 = self.check_onebyone(X, Y, eta, loss_fprop, self.b2, (1, self.m))\n",
    "ratio_b2 = np.nanmean(finite_grad_b2/self.grad_b2.reshape((1, self.m)))\n",
    "\n",
    "check = True\n",
    "ratios = [ratio_W1 ,ratio_W2, ratio_b1, ratio_b2]\n",
    "\n",
    "for ratio in ratios:\n",
    "    check = check and ratio < 1.01 and ratio > 0.99\n",
    "\n",
    "if (verbose):\n",
    "    print(\"Backprop gradient W1\")\n",
    "    print(np.nan_to_num(self.grad_W1))\n",
    "    print(\"Finite gradient W1\")\n",
    "    print(np.nan_to_num(finite_grad_W1))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient b1\")\n",
    "    print(np.nan_to_num(self.grad_b1))\n",
    "    print(\"Finite gradient b1\")\n",
    "    print(np.nan_to_num(finite_grad_b1))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient W2\")\n",
    "    print(np.nan_to_num(self.grad_W2))\n",
    "    print(\"Finite gradient W2\")\n",
    "    print(np.nan_to_num(finite_grad_W2))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient b2\")\n",
    "    print(np.nan_to_num(self.grad_b2))\n",
    "    print(\"Finite gradient b2\")\n",
    "    print(np.nan_to_num(finite_grad_b2))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(f\" ratio W1 : {ratio_W1}  \\n ratio b1 : {ratio_b1} \\\n",
    "          \\n ratio W2 : {ratio_W2} \\n ratio b2 : {ratio_b2}\")\n",
    "\n",
    "return check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
