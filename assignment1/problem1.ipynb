{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "f = gzip.open('data/mnist.pkl.gz') \n",
    "mnist = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features = mnist[0][0] #(50000,784)\n",
    "mnist_train_labels = mnist[0][1]   #(50000,1)\n",
    "\n",
    "mnist_valid_features = mnist[1][0] #(10000,784)\n",
    "mnist_valid_labels = mnist[1][1]   #(50000,1)\n",
    "\n",
    "mnist_test_features = mnist[2][0]  #(10000,784)\n",
    "mnist_test_labels = mnist[2][1]    #(50000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataset size for testing, if needed (ex: X_train = mnist_train_features[0:10000])\n",
    "X_train = mnist_train_features[0:3000]\n",
    "y_train = mnist_train_labels[0:3000]\n",
    "\n",
    "X_valid = mnist_valid_features[0:300]\n",
    "y_valid = mnist_valid_labels[0:300]\n",
    "\n",
    "X_test = mnist_test_features[0:300]\n",
    "y_test = mnist_test_labels[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,hidden_dims=(1024,2048),weight_mode=None, mode=\"ReLU\"):\n",
    "        self.b1 = np.zeros((hidden_dims[0]))\n",
    "        self.b2 = np.zeros((hidden_dims[1]))\n",
    "        self.b3 = np.zeros((10))\n",
    "        self.learning_rate = 0.0005\n",
    "        self.initialize_weights(hidden_dims,weight_mode)\n",
    "        self.act_mode = mode\n",
    "\n",
    "    def initialize_weights(self,hidden_dims,weight_mode):\n",
    "        W1_dim = (784, hidden_dims[0])\n",
    "        W2_dim = (hidden_dims[0], hidden_dims[1])\n",
    "        W3_dim = (hidden_dims[1], 10)\n",
    "        \n",
    "        if weight_mode == \"zero\":\n",
    "            self.W1 = np.zeros(W1_dim)\n",
    "            self.W2 = np.zeros(W2_dim)\n",
    "            self.W3 = np.zeros(W3_dim)\n",
    "            \n",
    "        elif weight_mode == \"normal\":\n",
    "            np.random.seed(23)\n",
    "            self.W1 = np.random.standard_normal(W1_dim)\n",
    "            self.W2 = np.random.standard_normal(W2_dim)\n",
    "            self.W3 = np.random.standard_normal(W3_dim)\n",
    "            \n",
    "        elif weight_mode == \"glorot\":\n",
    "            np.random.seed(23)\n",
    "            \n",
    "            d1 = math.sqrt(6.0 / (W1_dim[0] + W1_dim[1]))\n",
    "            d2 = math.sqrt(6.0 / (W2_dim[0] + W2_dim[1]))\n",
    "            d3 = math.sqrt(6.0 / (W3_dim[0] + W3_dim[1]))\n",
    "            \n",
    "            self.W1 = np.random.uniform(-d1, d1, W1_dim)\n",
    "            self.W2 = np.random.uniform(-d2, d2, W2_dim)\n",
    "            self.W3 = np.random.uniform(-d3, d3, W3_dim)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Unsupported weight_mode value\")\n",
    "\n",
    "    def forward(self,data):\n",
    "        cache = {}\n",
    "        \n",
    "        cache[\"h0\"] = np.copy(data)\n",
    "        cache[\"a1\"] = np.dot(cache[\"h0\"], self.W1) + self.b1\n",
    "        cache[\"h1\"] = self.activation(cache[\"a1\"], self.act_mode)\n",
    "        cache[\"a2\"] = np.dot(cache[\"h1\"], self.W2) + self.b2\n",
    "        cache[\"h2\"] = self.activation(cache[\"a2\"], self.act_mode)\n",
    "        cache[\"a3\"] = np.dot(cache[\"h2\"], self.W3) + self.b3\n",
    "        cache[\"h3\"] = self.softmax(cache[\"a3\"])\n",
    "        \n",
    "        return cache\n",
    "\n",
    "    def activation(self,data, mode=\"ReLU\"):\n",
    "        if(mode == \"ReLU\"):\n",
    "            return self.ReLU(data)\n",
    "        elif(mode == \"sigmoid\"):\n",
    "            return self.sigmoid(data)\n",
    "        else:\n",
    "            raise Exception(\"Not an activation function\")\n",
    "    \n",
    "    def activation_derivative(self,data, mode=\"ReLU\"): # RELU derivative\n",
    "        if(mode == \"ReLU\"):\n",
    "            return self.ReLU_derivative(data)\n",
    "        elif(mode == \"sigmoid\"):\n",
    "            return self.sigmoid_derivative(data)\n",
    "        else:\n",
    "            raise Exception(\"Not an activation function\")\n",
    "    \n",
    "    def sigmoid(self, data):\n",
    "        return 1/(1+np.exp(-data))\n",
    "    \n",
    "    def sigmoid_derivative(self,data):\n",
    "        return self.sigmoid(data)*(1-self.sigmoid(data))\n",
    "    \n",
    "    def ReLU(self,data): # RELU\n",
    "        return np.where(data > 0, data, 0.0)\n",
    "    \n",
    "    def ReLU_derivative(self,data):\n",
    "        return np.where(data > 0, 1.0, 0.0)\n",
    "\n",
    "    def loss(self, data, labels, eps = 1e-15): # Cross Entropy loss\n",
    "        cache = self.forward(data)\n",
    "        preds = np.clip(cache[\"h3\"], eps, 1 - eps)\n",
    "        preds = preds / np.sum(preds, axis=1)[:, np.newaxis]\n",
    "        one_hot_truth = np.eye(10)[labels]\n",
    "        log_loss = -np.diag(np.matmul(one_hot_truth,np.log(preds).T))\n",
    "        return  np.average(log_loss)\n",
    "#         return metrics.log_loss(labels, preds)\n",
    "\n",
    "    def softmax(self,data): # Numerically stable softmax\n",
    "        result = np.exp(data - np.max(data, axis=1, keepdims=True))\n",
    "        result = result / np.sum(result, axis=1, keepdims=True)\n",
    "        return result\n",
    "\n",
    "    def backward(self,cache,labels):\n",
    "        grads = {}\n",
    "\n",
    "        grads[\"a3\"] = cache[\"h3\"] - np.eye(10)[labels]\n",
    "        grads[\"W3\"] = np.dot(cache[\"h2\"].T, grads[\"a3\"])\n",
    "        grads[\"b3\"] = np.sum(grads[\"a3\"], axis=0)\n",
    "        \n",
    "        grads[\"h2\"] = np.dot(grads[\"a3\"], self.W3.T)\n",
    "        grads[\"a2\"] = grads[\"h2\"] * self.activation_derivative(cache[\"a2\"], self.act_mode)\n",
    "        grads[\"W2\"] = np.dot(cache[\"h1\"].T, grads[\"a2\"])\n",
    "        grads[\"b2\"] = np.sum(grads[\"a2\"], axis=0)\n",
    "        \n",
    "        grads[\"h1\"] = np.dot(grads[\"a2\"], self.W2.T)\n",
    "        grads[\"a1\"] = grads[\"h1\"] * self.activation_derivative(cache[\"a1\"], self.act_mode)\n",
    "        grads[\"W1\"] = np.dot(cache[\"h0\"].T, grads[\"a1\"])\n",
    "        grads[\"b1\"] = np.sum(grads[\"a1\"], axis=0)\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def update(self,grads):\n",
    "        self.b3 -= (self.learning_rate * grads[\"b3\"])\n",
    "        self.W3 -= (self.learning_rate * grads[\"W3\"])\n",
    "        self.b2 -= (self.learning_rate * grads[\"b2\"])\n",
    "        self.W2 -= (self.learning_rate * grads[\"W2\"])\n",
    "        self.b1 -= (self.learning_rate * grads[\"b1\"])\n",
    "        self.W1 -= (self.learning_rate * grads[\"W1\"])\n",
    "\n",
    "    def train(self,data,labels, mini_batch_size = 0, epochs = 10, learning_rate = 0.0005, lr_decay = 1, lr_decay_intervals = 0):\n",
    "        self.learning_rate = learning_rate\n",
    "        train_info = {}\n",
    "        train_info[\"average_losses\"] = []\n",
    "        train_info[\"accuracy\"]=[]\n",
    "        decay_flag = True\n",
    "        \n",
    "        # print(\"Epoch\\tAccuracy\\tAverage Loss\")\n",
    "        data_mini = []\n",
    "        label_mini = []\n",
    "        \n",
    "        if (mini_batch_size==0):\n",
    "            mini_batch_size = np.shape(data)[0]\n",
    "            \n",
    "        if (lr_decay==1):\n",
    "            decay_flag = False\n",
    "            \n",
    "        if (lr_decay>1):\n",
    "            raise ValueError(\"lr_decay has to be equal or under to 1\")\n",
    "            \n",
    "        if (decay_flag==True):\n",
    "            if lr_decay_intervals!=0:\n",
    "                lr_decay_intervals = lr_decay_intervals\n",
    "            else:\n",
    "                lr_decay_intervals = [0.2,0.6]\n",
    "        \n",
    "        for epoch in range(epochs): # TODO: implement mini-batch gradient descent\n",
    "            for i in range(0, np.shape(data)[0], mini_batch_size):\n",
    "                \n",
    "                #Mini-batch training\n",
    "                data_mini = data[i:i + mini_batch_size]\n",
    "                labels_mini = labels[i:i + mini_batch_size]\n",
    "                \n",
    "                #forward and backward propagation\n",
    "                cache = self.forward(data_mini)\n",
    "                grads = self.backward(cache,labels_mini)\n",
    "                self.update(grads)\n",
    "                \n",
    "                #Measuring loss and accuracy\n",
    "                predictions = np.argmax(cache[\"h3\"], axis=1)\n",
    "                accuracy = np.mean(predictions == labels_mini)\n",
    "                average_loss = self.loss(data_mini, labels_mini)\n",
    "\n",
    "            if(decay_flag):\n",
    "                if (epoch in np.dot(lr_decay_intervals,epochs)):\n",
    "                    learning_rate = learning_rate * lr_decay\n",
    "                    print(learning_rate)\n",
    "            \n",
    "            train_info[\"average_losses\"].append(average_loss)\n",
    "            train_info[\"accuracy\"].append(accuracy)    \n",
    "            print(epoch + 1, \"\\t\", \"{0:.4f}\".format(accuracy), \"\\t\", \"{0:.4f}\".format(average_loss))\n",
    "        \n",
    "        return train_info\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        cache = self.forward(data)\n",
    "        predictions = cache[\"h3\"].argmax(1).astype(int)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero\n",
      "1 \t 0.1300 \t 2.3025\n",
      "2 \t 0.1300 \t 2.3028\n",
      "0.0005\n",
      "3 \t 0.1300 \t 2.3031\n",
      "4 \t 0.1300 \t 2.3035\n",
      "5 \t 0.1300 \t 2.3038\n",
      "6 \t 0.1300 \t 2.3041\n",
      "0.00025\n",
      "7 \t 0.1300 \t 2.3043\n",
      "8 \t 0.1300 \t 2.3045\n",
      "9 \t 0.1300 \t 2.3046\n",
      "10 \t 0.1300 \t 2.3047\n",
      "\n",
      "normal\n",
      "1 \t 0.1000 \t 2.3170\n",
      "2 \t 0.1300 \t 2.3102\n",
      "0.0005\n",
      "3 \t 0.1300 \t 2.3065\n",
      "4 \t 0.1300 \t 2.3047\n",
      "5 \t 0.1300 \t 2.3039\n",
      "6 \t 0.1300 \t 2.3037\n",
      "0.00025\n",
      "7 \t 0.1300 \t 2.3037\n",
      "8 \t 0.1300 \t 2.3039\n",
      "9 \t 0.1300 \t 2.3041\n",
      "10 \t 0.1300 \t 2.3043\n",
      "\n",
      "glorot\n",
      "1 \t 0.8400 \t 0.6415\n",
      "2 \t 0.8800 \t 0.4075\n",
      "0.0005\n",
      "3 \t 0.8900 \t 0.3190\n",
      "4 \t 0.9100 \t 0.2624\n",
      "5 \t 0.9200 \t 0.2200\n",
      "6 \t 0.9300 \t 0.1873\n",
      "0.00025\n",
      "7 \t 0.9400 \t 0.1615\n",
      "8 \t 0.9400 \t 0.1401\n",
      "9 \t 0.9600 \t 0.1233\n",
      "10 \t 0.9700 \t 0.1094\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for weight_mode in [\"zero\", \"normal\", \"glorot\"]:\n",
    "#for weight_mode in [\"glorot\"]:\n",
    "    print(weight_mode)\n",
    "    nn = NN(hidden_dims=(800,300),weight_mode=weight_mode, mode=\"ReLU\")\n",
    "    train_info = nn.train(X_train, y_train, epochs = 10, mini_batch_size=100, learning_rate = 0.001, lr_decay = 0.5)\n",
    "    plt.plot(np.arange(1,11,1), train_info[\"average_losses\"], label=weight_mode)\n",
    "    print()\n",
    "\n",
    "plt.title(\"Average loss by epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0.9107 \t 0.2318\n",
      "2 \t 0.9286 \t 0.1235\n",
      "3 \t 0.9464 \t 0.0633\n",
      "4 \t 0.9643 \t 0.0475\n",
      "0.008\n",
      "5 \t 0.9643 \t 0.0316\n",
      "6 \t 0.9821 \t 0.0168\n",
      "7 \t 0.9821 \t 0.0183\n",
      "8 \t 1.0000 \t 0.0057\n",
      "9 \t 1.0000 \t 0.0029\n",
      "10 \t 1.0000 \t 0.0023\n",
      "0.0064\n",
      "11 \t 1.0000 \t 0.0020\n",
      "12 \t 1.0000 \t 0.0017\n",
      "13 \t 1.0000 \t 0.0015\n",
      "14 \t 1.0000 \t 0.0013\n",
      "15 \t 1.0000 \t 0.0012\n",
      "16 \t 1.0000 \t 0.0011\n",
      "0.00512\n",
      "17 \t 1.0000 \t 0.0010\n",
      "18 \t 1.0000 \t 0.0009\n",
      "19 \t 1.0000 \t 0.0008\n",
      "20 \t 1.0000 \t 0.0008\n",
      "1 \t 0.9107 \t 0.2318\n",
      "2 \t 0.9286 \t 0.1235\n",
      "3 \t 0.9464 \t 0.0633\n",
      "4 \t 0.9643 \t 0.0475\n",
      "5 \t 0.9643 \t 0.0316\n",
      "6 \t 0.9821 \t 0.0168\n",
      "0.008\n",
      "7 \t 0.9821 \t 0.0183\n",
      "8 \t 1.0000 \t 0.0057\n",
      "9 \t 1.0000 \t 0.0029\n",
      "10 \t 1.0000 \t 0.0023\n",
      "11 \t 1.0000 \t 0.0020\n",
      "12 \t 1.0000 \t 0.0017\n",
      "13 \t 1.0000 \t 0.0015\n",
      "14 \t 1.0000 \t 0.0013\n",
      "15 \t 1.0000 \t 0.0012\n",
      "0.0064\n",
      "16 \t 1.0000 \t 0.0011\n",
      "17 \t 1.0000 \t 0.0010\n",
      "18 \t 1.0000 \t 0.0009\n",
      "19 \t 1.0000 \t 0.0008\n",
      "20 \t 1.0000 \t 0.0008\n",
      "21 \t 1.0000 \t 0.0007\n",
      "22 \t 1.0000 \t 0.0007\n",
      "23 \t 1.0000 \t 0.0006\n",
      "24 \t 1.0000 \t 0.0006\n",
      "0.00512\n",
      "25 \t 1.0000 \t 0.0006\n",
      "26 \t 1.0000 \t 0.0005\n",
      "27 \t 1.0000 \t 0.0005\n",
      "28 \t 1.0000 \t 0.0005\n",
      "29 \t 1.0000 \t 0.0005\n",
      "30 \t 1.0000 \t 0.0005\n",
      "1 \t 0.9107 \t 0.2318\n",
      "2 \t 0.9286 \t 0.1235\n",
      "3 \t 0.9464 \t 0.0633\n",
      "4 \t 0.9643 \t 0.0475\n",
      "5 \t 0.9643 \t 0.0316\n",
      "6 \t 0.9821 \t 0.0168\n",
      "7 \t 0.9821 \t 0.0183\n",
      "8 \t 1.0000 \t 0.0057\n",
      "0.008\n",
      "9 \t 1.0000 \t 0.0029\n",
      "10 \t 1.0000 \t 0.0023\n",
      "11 \t 1.0000 \t 0.0020\n",
      "12 \t 1.0000 \t 0.0017\n",
      "13 \t 1.0000 \t 0.0015\n",
      "14 \t 1.0000 \t 0.0013\n",
      "15 \t 1.0000 \t 0.0012\n",
      "16 \t 1.0000 \t 0.0011\n",
      "17 \t 1.0000 \t 0.0010\n",
      "18 \t 1.0000 \t 0.0009\n",
      "19 \t 1.0000 \t 0.0008\n",
      "20 \t 1.0000 \t 0.0008\n",
      "0.0064\n",
      "21 \t 1.0000 \t 0.0007\n",
      "22 \t 1.0000 \t 0.0007\n",
      "23 \t 1.0000 \t 0.0006\n",
      "24 \t 1.0000 \t 0.0006\n",
      "25 \t 1.0000 \t 0.0006\n",
      "26 \t 1.0000 \t 0.0005\n",
      "27 \t 1.0000 \t 0.0005\n",
      "28 \t 1.0000 \t 0.0005\n",
      "29 \t 1.0000 \t 0.0005\n",
      "30 \t 1.0000 \t 0.0005\n",
      "31 \t 1.0000 \t 0.0004\n",
      "32 \t 1.0000 \t 0.0004\n",
      "0.00512\n",
      "33 \t 1.0000 \t 0.0004\n",
      "34 \t 1.0000 \t 0.0004\n",
      "35 \t 1.0000 \t 0.0004\n",
      "36 \t 1.0000 \t 0.0004\n",
      "37 \t 1.0000 \t 0.0003\n",
      "38 \t 1.0000 \t 0.0003\n",
      "39 \t 1.0000 \t 0.0003\n",
      "40 \t 1.0000 \t 0.0003\n",
      "1 \t 0.9107 \t 0.2457\n",
      "2 \t 0.8929 \t 0.1462\n",
      "3 \t 0.9464 \t 0.0608\n",
      "4 \t 0.9643 \t 0.0518\n",
      "0.0064\n",
      "5 \t 0.9643 \t 0.0255\n",
      "6 \t 1.0000 \t 0.0139\n",
      "7 \t 1.0000 \t 0.0092\n",
      "8 \t 1.0000 \t 0.0060\n",
      "9 \t 1.0000 \t 0.0041\n",
      "10 \t 1.0000 \t 0.0032\n",
      "0.00512\n",
      "11 \t 1.0000 \t 0.0027\n",
      "12 \t 1.0000 \t 0.0024\n",
      "13 \t 1.0000 \t 0.0021\n",
      "14 \t 1.0000 \t 0.0019\n",
      "15 \t 1.0000 \t 0.0017\n",
      "16 \t 1.0000 \t 0.0016\n",
      "0.004096000000000001\n",
      "17 \t 1.0000 \t 0.0015\n",
      "18 \t 1.0000 \t 0.0014\n",
      "19 \t 1.0000 \t 0.0013\n",
      "20 \t 1.0000 \t 0.0012\n",
      "1 \t 0.9107 \t 0.2457\n",
      "2 \t 0.8929 \t 0.1462\n",
      "3 \t 0.9464 \t 0.0608\n",
      "4 \t 0.9643 \t 0.0518\n",
      "5 \t 0.9643 \t 0.0255\n",
      "6 \t 1.0000 \t 0.0139\n",
      "0.0064\n",
      "7 \t 1.0000 \t 0.0092\n",
      "8 \t 1.0000 \t 0.0060\n",
      "9 \t 1.0000 \t 0.0041\n",
      "10 \t 1.0000 \t 0.0032\n",
      "11 \t 1.0000 \t 0.0027\n",
      "12 \t 1.0000 \t 0.0024\n",
      "13 \t 1.0000 \t 0.0021\n",
      "14 \t 1.0000 \t 0.0019\n",
      "15 \t 1.0000 \t 0.0017\n",
      "0.00512\n",
      "16 \t 1.0000 \t 0.0016\n",
      "17 \t 1.0000 \t 0.0015\n",
      "18 \t 1.0000 \t 0.0014\n",
      "19 \t 1.0000 \t 0.0013\n",
      "20 \t 1.0000 \t 0.0012\n",
      "21 \t 1.0000 \t 0.0011\n",
      "22 \t 1.0000 \t 0.0010\n",
      "23 \t 1.0000 \t 0.0010\n",
      "24 \t 1.0000 \t 0.0009\n",
      "0.004096000000000001\n",
      "25 \t 1.0000 \t 0.0009\n",
      "26 \t 1.0000 \t 0.0008\n",
      "27 \t 1.0000 \t 0.0008\n",
      "28 \t 1.0000 \t 0.0007\n",
      "29 \t 1.0000 \t 0.0007\n",
      "30 \t 1.0000 \t 0.0007\n",
      "1 \t 0.9107 \t 0.2457\n",
      "2 \t 0.8929 \t 0.1462\n",
      "3 \t 0.9464 \t 0.0608\n",
      "4 \t 0.9643 \t 0.0518\n",
      "5 \t 0.9643 \t 0.0255\n",
      "6 \t 1.0000 \t 0.0139\n",
      "7 \t 1.0000 \t 0.0092\n",
      "8 \t 1.0000 \t 0.0060\n",
      "0.0064\n",
      "9 \t 1.0000 \t 0.0041\n",
      "10 \t 1.0000 \t 0.0032\n",
      "11 \t 1.0000 \t 0.0027\n",
      "12 \t 1.0000 \t 0.0024\n",
      "13 \t 1.0000 \t 0.0021\n",
      "14 \t 1.0000 \t 0.0019\n",
      "15 \t 1.0000 \t 0.0017\n",
      "16 \t 1.0000 \t 0.0016\n",
      "17 \t 1.0000 \t 0.0015\n",
      "18 \t 1.0000 \t 0.0014\n",
      "19 \t 1.0000 \t 0.0013\n",
      "20 \t 1.0000 \t 0.0012\n",
      "0.00512\n",
      "21 \t 1.0000 \t 0.0011\n",
      "22 \t 1.0000 \t 0.0010\n",
      "23 \t 1.0000 \t 0.0010\n",
      "24 \t 1.0000 \t 0.0009\n",
      "25 \t 1.0000 \t 0.0009\n",
      "26 \t 1.0000 \t 0.0008\n",
      "27 \t 1.0000 \t 0.0008\n",
      "28 \t 1.0000 \t 0.0007\n",
      "29 \t 1.0000 \t 0.0007\n",
      "30 \t 1.0000 \t 0.0007\n",
      "31 \t 1.0000 \t 0.0006\n",
      "32 \t 1.0000 \t 0.0006\n",
      "0.004096000000000001\n",
      "33 \t 1.0000 \t 0.0006\n",
      "34 \t 1.0000 \t 0.0006\n",
      "35 \t 1.0000 \t 0.0005\n",
      "36 \t 1.0000 \t 0.0005\n",
      "37 \t 1.0000 \t 0.0005\n",
      "38 \t 1.0000 \t 0.0005\n",
      "39 \t 1.0000 \t 0.0005\n",
      "40 \t 1.0000 \t 0.0005\n",
      "1 \t 0.7321 \t 0.5627\n",
      "2 \t 0.8929 \t 0.2158\n",
      "3 \t 0.8571 \t 0.1816\n",
      "4 \t 0.9107 \t 0.1397\n",
      "0.009600000000000001\n",
      "5 \t 0.9464 \t 0.0653\n",
      "6 \t 0.9464 \t 0.0920\n",
      "7 \t 0.9821 \t 0.0113\n",
      "8 \t 1.0000 \t 0.0059\n",
      "9 \t 0.9821 \t 0.0120\n",
      "10 \t 0.9821 \t 0.0039\n",
      "0.007680000000000001\n",
      "11 \t 1.0000 \t 0.0015\n",
      "12 \t 1.0000 \t 0.0011\n",
      "13 \t 1.0000 \t 0.0012\n",
      "14 \t 1.0000 \t 0.0011\n",
      "15 \t 1.0000 \t 0.0009\n",
      "16 \t 1.0000 \t 0.0008\n",
      "0.006144000000000001\n",
      "17 \t 1.0000 \t 0.0007\n",
      "18 \t 1.0000 \t 0.0006\n",
      "19 \t 1.0000 \t 0.0005\n",
      "20 \t 1.0000 \t 0.0005\n",
      "1 \t 0.7321 \t 0.5627\n",
      "2 \t 0.8929 \t 0.2158\n",
      "3 \t 0.8571 \t 0.1816\n",
      "4 \t 0.9107 \t 0.1397\n",
      "5 \t 0.9464 \t 0.0653\n",
      "6 \t 0.9464 \t 0.0920\n",
      "0.009600000000000001\n",
      "7 \t 0.9821 \t 0.0113\n",
      "8 \t 1.0000 \t 0.0059\n",
      "9 \t 0.9821 \t 0.0120\n",
      "10 \t 0.9821 \t 0.0039\n",
      "11 \t 1.0000 \t 0.0015\n",
      "12 \t 1.0000 \t 0.0011\n",
      "13 \t 1.0000 \t 0.0012\n",
      "14 \t 1.0000 \t 0.0011\n",
      "15 \t 1.0000 \t 0.0009\n",
      "0.007680000000000001\n",
      "16 \t 1.0000 \t 0.0008\n",
      "17 \t 1.0000 \t 0.0007\n",
      "18 \t 1.0000 \t 0.0006\n",
      "19 \t 1.0000 \t 0.0005\n",
      "20 \t 1.0000 \t 0.0005\n",
      "21 \t 1.0000 \t 0.0005\n",
      "22 \t 1.0000 \t 0.0004\n",
      "23 \t 1.0000 \t 0.0004\n",
      "24 \t 1.0000 \t 0.0004\n",
      "0.006144000000000001\n",
      "25 \t 1.0000 \t 0.0003\n",
      "26 \t 1.0000 \t 0.0003\n",
      "27 \t 1.0000 \t 0.0003\n",
      "28 \t 1.0000 \t 0.0003\n",
      "29 \t 1.0000 \t 0.0003\n",
      "30 \t 1.0000 \t 0.0003\n",
      "1 \t 0.7321 \t 0.5627\n",
      "2 \t 0.8929 \t 0.2158\n",
      "3 \t 0.8571 \t 0.1816\n",
      "4 \t 0.9107 \t 0.1397\n",
      "5 \t 0.9464 \t 0.0653\n",
      "6 \t 0.9464 \t 0.0920\n",
      "7 \t 0.9821 \t 0.0113\n",
      "8 \t 1.0000 \t 0.0059\n",
      "0.009600000000000001\n",
      "9 \t 0.9821 \t 0.0120\n",
      "10 \t 0.9821 \t 0.0039\n",
      "11 \t 1.0000 \t 0.0015\n",
      "12 \t 1.0000 \t 0.0011\n",
      "13 \t 1.0000 \t 0.0012\n",
      "14 \t 1.0000 \t 0.0011\n",
      "15 \t 1.0000 \t 0.0009\n",
      "16 \t 1.0000 \t 0.0008\n",
      "17 \t 1.0000 \t 0.0007\n",
      "18 \t 1.0000 \t 0.0006\n",
      "19 \t 1.0000 \t 0.0005\n",
      "20 \t 1.0000 \t 0.0005\n",
      "0.007680000000000001\n",
      "21 \t 1.0000 \t 0.0005\n",
      "22 \t 1.0000 \t 0.0004\n",
      "23 \t 1.0000 \t 0.0004\n",
      "24 \t 1.0000 \t 0.0004\n",
      "25 \t 1.0000 \t 0.0003\n",
      "26 \t 1.0000 \t 0.0003\n",
      "27 \t 1.0000 \t 0.0003\n",
      "28 \t 1.0000 \t 0.0003\n",
      "29 \t 1.0000 \t 0.0003\n",
      "30 \t 1.0000 \t 0.0003\n",
      "31 \t 1.0000 \t 0.0003\n",
      "32 \t 1.0000 \t 0.0002\n",
      "0.006144000000000001\n",
      "33 \t 1.0000 \t 0.0002\n",
      "34 \t 1.0000 \t 0.0002\n",
      "35 \t 1.0000 \t 0.0002\n",
      "36 \t 1.0000 \t 0.0002\n",
      "37 \t 1.0000 \t 0.0002\n",
      "38 \t 1.0000 \t 0.0002\n",
      "39 \t 1.0000 \t 0.0002\n",
      "40 \t 1.0000 \t 0.0002\n",
      "0.9466666666666667 {'lr': 0.01, 'hd': (625, 625), 'mb': 64, 'epoch': 20}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "\n",
    "lr = [0.01, 0.008, 0.012]\n",
    "hiddens_dims = [(625,625)]\n",
    "mini_batch = [64]\n",
    "epoch = (20,30,40)\n",
    "\n",
    "\n",
    "# Mode = ReLU\n",
    "best = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr_ in lr:\n",
    "    for hd in hiddens_dims:\n",
    "        for mb in mini_batch:\n",
    "            for epoch_ in epoch:\n",
    "                #print(\"Lr : {}, hd : {}, mb : {}\".format(lr_, hd, mb))\n",
    "                model = NN(hidden_dims = hd, weight_mode = \"glorot\", mode = \"ReLU\")\n",
    "                train_log = model.train(X_train, y_train, epochs = epoch_,\n",
    "                                        mini_batch_size=mb, learning_rate = lr_, lr_decay = 0.8,\n",
    "                                        lr_decay_intervals = [0.2, 0.5, 0.8])\n",
    "                acc = model.test(X_valid, y_valid)\n",
    "                if(acc>best):\n",
    "                    best = acc\n",
    "                    best_params[\"lr\"] = lr_\n",
    "                    best_params[\"hd\"] = hd\n",
    "                    best_params[\"mb\"] = mb\n",
    "                    best_params[\"epoch\"]=epoch_\n",
    "                              \n",
    "print(best, best_params)\n",
    "\n",
    "# # Mode = sigmoid\n",
    "# best_s = 0\n",
    "# best_params_s = {}\n",
    "\n",
    "# for lr_ in lr:\n",
    "#     for hd in hiddens_dims:\n",
    "#         for mb in mini_batch:\n",
    "#             for epoch_ in epoch:\n",
    "#                 #print(\"Lr : {}, hd : {}, mb : {}\".format(lr_, hd, mb))\n",
    "#                 model = NN(hidden_dims = hd, weight_mode = \"glorot\", mode = \"sigmoid\")\n",
    "#                 train_log = model.train(X_train, y_train, epochs = epoch_, mini_batch_size=mb, learning_rate = lr_)\n",
    "#                 acc = model.test(X_valid, y_valid)\n",
    "#                 if(acc>best_s):\n",
    "#                     best_s = acc\n",
    "#                     best_params_s[\"lr\"] = lr_\n",
    "#                     best_params_s[\"hd\"] = hd\n",
    "#                     best_params_s[\"mb\"] = mb\n",
    "#                     best_params_s[\"epoch\"]=epoch_\n",
    "                              \n",
    "# print(best_s, best_params_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Gradients Using Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from Marc-Andr√©'s homework#3 of IFT6390 class.\n",
    "\n",
    "def finite_diff_gradient_check(self,X, Y, verbose = False):\n",
    "\"\"\"\n",
    "-----\n",
    "Parameters: \n",
    "    X : inputs of dimension d for 1 example o , todo, do for mini batch\n",
    "    onehot_Y : encoded target associated to X \n",
    "    Y : index of class\n",
    "\"\"\"\n",
    "X = X.reshape((1, X.shape[0]))\n",
    "self.fprop(X)\n",
    "onehot_Y = to_categorical(Y, num_classes=self.m).reshape((1, self.m))\n",
    "self.bprop(X,onehot_Y)\n",
    "\n",
    "loss_fprop = self.loss\n",
    "\n",
    "eta = 10**(-5)\n",
    "\n",
    "finite_grad_W1 = self.check_onebyone(X, Y, eta, loss_fprop, self.W1, (self.dh, self.d))\n",
    "ratio_W1 = np.nanmean(finite_grad_W1/self.grad_W1)\n",
    "\n",
    "finite_grad_W2 = self.check_onebyone(X, Y, eta, loss_fprop, self.W2, (self.m, self.dh))\n",
    "ratio_W2 = np.nanmean(finite_grad_W2/self.grad_W2)\n",
    "\n",
    "finite_grad_b1 = self.check_onebyone(X, Y, eta, loss_fprop, self.b1, (1, self.dh))\n",
    "ratio_b1 = np.nanmean(finite_grad_b1/self.grad_b1.reshape((1, self.dh)))\n",
    "\n",
    "finite_grad_b2 = self.check_onebyone(X, Y, eta, loss_fprop, self.b2, (1, self.m))\n",
    "ratio_b2 = np.nanmean(finite_grad_b2/self.grad_b2.reshape((1, self.m)))\n",
    "\n",
    "check = True\n",
    "ratios = [ratio_W1 ,ratio_W2, ratio_b1, ratio_b2]\n",
    "\n",
    "for ratio in ratios:\n",
    "    check = check and ratio < 1.01 and ratio > 0.99\n",
    "\n",
    "if (verbose):\n",
    "    print(\"Backprop gradient W1\")\n",
    "    print(np.nan_to_num(self.grad_W1))\n",
    "    print(\"Finite gradient W1\")\n",
    "    print(np.nan_to_num(finite_grad_W1))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient b1\")\n",
    "    print(np.nan_to_num(self.grad_b1))\n",
    "    print(\"Finite gradient b1\")\n",
    "    print(np.nan_to_num(finite_grad_b1))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient W2\")\n",
    "    print(np.nan_to_num(self.grad_W2))\n",
    "    print(\"Finite gradient W2\")\n",
    "    print(np.nan_to_num(finite_grad_W2))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient b2\")\n",
    "    print(np.nan_to_num(self.grad_b2))\n",
    "    print(\"Finite gradient b2\")\n",
    "    print(np.nan_to_num(finite_grad_b2))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(f\" ratio W1 : {ratio_W1}  \\n ratio b1 : {ratio_b1} \\\n",
    "          \\n ratio W2 : {ratio_W2} \\n ratio b2 : {ratio_b2}\")\n",
    "\n",
    "return check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
