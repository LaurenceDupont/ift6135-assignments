{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "f = gzip.open('data/mnist.pkl.gz') \n",
    "mnist = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features = mnist[0][0] #(50000,784)\n",
    "mnist_train_labels = mnist[0][1]   #(50000,1)\n",
    "\n",
    "mnist_valid_features = mnist[1][0] #(10000,784)\n",
    "mnist_valid_labels = mnist[1][1]   #(50000,1)\n",
    "\n",
    "mnist_test_features = mnist[2][0]  #(10000,784)\n",
    "mnist_test_labels = mnist[2][1]    #(50000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dataset size for testing, if needed (ex: X_train = mnist_train_features[0:10000])\n",
    "X_train = mnist_train_features[0:50000]\n",
    "y_train = mnist_train_labels[0:50000]\n",
    "\n",
    "X_valid = mnist_valid_features[0:10000]\n",
    "y_valid = mnist_valid_labels[0:10000]\n",
    "\n",
    "X_test = mnist_test_features[0:300]\n",
    "y_test = mnist_test_labels[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,hidden_dims=(1024,2048),weight_mode=None, mode=\"ReLU\"):\n",
    "        self.b1 = np.zeros((hidden_dims[0]))\n",
    "        self.b2 = np.zeros((hidden_dims[1]))\n",
    "        self.b3 = np.zeros((10))\n",
    "        self.learning_rate = 0.0005\n",
    "        self.initialize_weights(hidden_dims,weight_mode)\n",
    "        self.grads = 0\n",
    "        self.act_mode = mode\n",
    "\n",
    "    def initialize_weights(self,hidden_dims,weight_mode):\n",
    "        W1_dim = (784, hidden_dims[0])\n",
    "        W2_dim = (hidden_dims[0], hidden_dims[1])\n",
    "        W3_dim = (hidden_dims[1], 10)\n",
    "        \n",
    "        if weight_mode == \"zero\":\n",
    "            self.W1 = np.zeros(W1_dim)\n",
    "            self.W2 = np.zeros(W2_dim)\n",
    "            self.W3 = np.zeros(W3_dim)\n",
    "            \n",
    "        elif weight_mode == \"normal\":\n",
    "            np.random.seed(23)\n",
    "            self.W1 = np.random.standard_normal(W1_dim)\n",
    "            self.W2 = np.random.standard_normal(W2_dim)\n",
    "            self.W3 = np.random.standard_normal(W3_dim)\n",
    "            \n",
    "        elif weight_mode == \"glorot\":\n",
    "            np.random.seed(23)\n",
    "            \n",
    "            d1 = math.sqrt(6.0 / (W1_dim[0] + W1_dim[1]))\n",
    "            d2 = math.sqrt(6.0 / (W2_dim[0] + W2_dim[1]))\n",
    "            d3 = math.sqrt(6.0 / (W3_dim[0] + W3_dim[1]))\n",
    "            \n",
    "            self.W1 = np.random.uniform(-d1, d1, W1_dim)\n",
    "            self.W2 = np.random.uniform(-d2, d2, W2_dim)\n",
    "            self.W3 = np.random.uniform(-d3, d3, W3_dim)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Unsupported weight_mode value\")\n",
    "\n",
    "    def forward(self,data):\n",
    "        cache = {}\n",
    "        \n",
    "        cache[\"h0\"] = np.copy(data)\n",
    "        cache[\"a1\"] = np.dot(cache[\"h0\"], self.W1) + self.b1\n",
    "        cache[\"h1\"] = self.activation(cache[\"a1\"], self.act_mode)\n",
    "        cache[\"a2\"] = np.dot(cache[\"h1\"], self.W2) + self.b2\n",
    "        cache[\"h2\"] = self.activation(cache[\"a2\"], self.act_mode)\n",
    "        cache[\"a3\"] = np.dot(cache[\"h2\"], self.W3) + self.b3\n",
    "        cache[\"h3\"] = self.softmax(cache[\"a3\"])\n",
    "        \n",
    "        return cache\n",
    "\n",
    "    def activation(self,data, mode=\"ReLU\"):\n",
    "        if(mode == \"ReLU\"):\n",
    "            return self.ReLU(data)\n",
    "        elif(mode == \"sigmoid\"):\n",
    "            return self.sigmoid(data)\n",
    "        else:\n",
    "            raise Exception(\"Not an activation function\")\n",
    "    \n",
    "    def activation_derivative(self,data, mode=\"ReLU\"): # RELU derivative\n",
    "        if(mode == \"ReLU\"):\n",
    "            return self.ReLU_derivative(data)\n",
    "        elif(mode == \"sigmoid\"):\n",
    "            return self.sigmoid_derivative(data)\n",
    "        else:\n",
    "            raise Exception(\"Not an activation function\")\n",
    "    \n",
    "    def sigmoid(self, data):\n",
    "        return 1/(1+np.exp(-data))\n",
    "    \n",
    "    def sigmoid_derivative(self,data):\n",
    "        return self.sigmoid(data)*(1-self.sigmoid(data))\n",
    "    \n",
    "    def ReLU(self,data): # RELU\n",
    "        return np.where(data > 0, data, 0.0)\n",
    "    \n",
    "    def ReLU_derivative(self,data):\n",
    "        return np.where(data > 0, 1.0, 0.0)\n",
    "\n",
    "    def loss(self, data, labels, eps = 1e-15): # Cross Entropy loss\n",
    "        cache = self.forward(data)\n",
    "        preds = np.clip(cache[\"h3\"], eps, 1 - eps)\n",
    "        preds = preds / np.sum(preds, axis=1)[:, np.newaxis]\n",
    "        one_hot_truth = np.eye(10)[labels]\n",
    "        log_loss = -np.diag(np.matmul(one_hot_truth,np.log(preds).T))\n",
    "        return  np.average(log_loss)\n",
    "#         return metrics.log_loss(labels, preds)\n",
    "\n",
    "    def softmax(self,data): # Numerically stable softmax\n",
    "        result = np.exp(data - np.max(data, axis=1, keepdims=True))\n",
    "        result = result / np.sum(result, axis=1, keepdims=True)\n",
    "        return result\n",
    "\n",
    "    def backward(self,cache,labels):\n",
    "        grads = {}\n",
    "\n",
    "        grads[\"a3\"] = cache[\"h3\"] - np.eye(10)[labels]\n",
    "        grads[\"W3\"] = np.dot(cache[\"h2\"].T, grads[\"a3\"])\n",
    "        grads[\"b3\"] = np.sum(grads[\"a3\"], axis=0)\n",
    "        \n",
    "        grads[\"h2\"] = np.dot(grads[\"a3\"], self.W3.T)\n",
    "        grads[\"a2\"] = grads[\"h2\"] * self.activation_derivative(cache[\"a2\"], self.act_mode)\n",
    "        grads[\"W2\"] = np.dot(cache[\"h1\"].T, grads[\"a2\"])\n",
    "        grads[\"b2\"] = np.sum(grads[\"a2\"], axis=0)\n",
    "        \n",
    "        grads[\"h1\"] = np.dot(grads[\"a2\"], self.W2.T)\n",
    "        grads[\"a1\"] = grads[\"h1\"] * self.activation_derivative(cache[\"a1\"], self.act_mode)\n",
    "        grads[\"W1\"] = np.dot(cache[\"h0\"].T, grads[\"a1\"])\n",
    "        grads[\"b1\"] = np.sum(grads[\"a1\"], axis=0)\n",
    "        \n",
    "        self.grads = grads\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def update(self,grads):\n",
    "        self.b3 -= (self.learning_rate * grads[\"b3\"])\n",
    "        self.W3 -= (self.learning_rate * grads[\"W3\"])\n",
    "        self.b2 -= (self.learning_rate * grads[\"b2\"])\n",
    "        self.W2 -= (self.learning_rate * grads[\"W2\"])\n",
    "        self.b1 -= (self.learning_rate * grads[\"b1\"])\n",
    "        self.W1 -= (self.learning_rate * grads[\"W1\"])\n",
    "\n",
    "    def train(self, train_data, train_labels, valid_data, valid_labels, mini_batch_size = 0, epochs = 10, learning_rate = 0.0005, lr_decay = 1, lr_decay_intervals = 0):\n",
    "        self.learning_rate = learning_rate\n",
    "        train_info = {}\n",
    "        train_info[\"train_loss\"] = []\n",
    "        train_info[\"train_accuracy\"] = []\n",
    "        train_info[\"valid_loss\"] = []\n",
    "        train_info[\"valid_accuracy\"] = []\n",
    "        decay_flag = True\n",
    "        \n",
    "        # print(\"Epoch\\tAccuracy\\tAverage Loss\")\n",
    "        data_mini = []\n",
    "        label_mini = []\n",
    "        \n",
    "        if (mini_batch_size==0):\n",
    "            mini_batch_size = np.shape(train_data)[0]\n",
    "            \n",
    "        if (lr_decay==1):\n",
    "            decay_flag = False\n",
    "            \n",
    "        if (lr_decay>1):\n",
    "            raise ValueError(\"lr_decay has to be equal or under to 1\")\n",
    "            \n",
    "        if (decay_flag==True):\n",
    "            if lr_decay_intervals!=0:\n",
    "                lr_decay_intervals = lr_decay_intervals\n",
    "            else:\n",
    "                lr_decay_intervals = [0.2,0.6]\n",
    "        \n",
    "        for epoch in range(epochs): # TODO: implement mini-batch gradient descent\n",
    "            for i in range(0, np.shape(train_data)[0], mini_batch_size):\n",
    "                \n",
    "                #Mini-batch training\n",
    "                data_mini = train_data[i:i + mini_batch_size]\n",
    "                labels_mini = train_labels[i:i + mini_batch_size]\n",
    "                \n",
    "                #forward and backward propagation\n",
    "                cache = self.forward(data_mini)\n",
    "                grads = self.backward(cache,labels_mini)\n",
    "                self.update(grads)\n",
    "                \n",
    "            #Measuring loss and accuracy\n",
    "            predictions = np.argmax(cache[\"h3\"], axis=1)\n",
    "            train_accuracy = np.mean(predictions == labels_mini)\n",
    "            train_loss = self.loss(data_mini, labels_mini)\n",
    "            \n",
    "            valid_cache = self.forward(valid_data)\n",
    "            valid_pred = np.argmax(valid_cache[\"h3\"], axis=1)\n",
    "            valid_accuracy = np.mean(valid_pred == valid_labels)\n",
    "            valid_loss = self.loss(valid_data, valid_labels)\n",
    "            \n",
    "            if(decay_flag):\n",
    "                if (epoch in np.dot(lr_decay_intervals,epochs)):\n",
    "                    learning_rate = learning_rate * lr_decay\n",
    "                    print(learning_rate)\n",
    "            \n",
    "            train_info[\"train_loss\"].append(train_loss)\n",
    "            train_info[\"train_accuracy\"].append(train_accuracy)   \n",
    "            train_info[\"valid_loss\"].append(valid_loss)\n",
    "            train_info[\"valid_accuracy\"].append(valid_accuracy)\n",
    "            print(\"Training epoch \", epoch + 1, \"\\t\", \"{0:.4f}\".format(train_accuracy), \"\\t\", \"{0:.4f}\".format(train_loss))\n",
    "            print(\"Validation \", \"\\t\", \"{0:.4f}\".format(valid_accuracy), \"\\t\", \"{0:.4f}\".format(valid_loss), '\\n')\n",
    "        \n",
    "        return train_info\n",
    "\n",
    "    def test(self, data, labels):\n",
    "        cache = self.forward(data)\n",
    "        predictions = cache[\"h3\"].argmax(1).astype(int)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero\n",
      "1 \t 0.1700 \t 2.2978\n",
      "2 \t 0.1700 \t 2.2972\n",
      "0.0005\n",
      "3 \t 0.1700 \t 2.2972\n",
      "4 \t 0.1700 \t 2.2972\n",
      "5 \t 0.1700 \t 2.2972\n",
      "6 \t 0.1700 \t 2.2972\n",
      "0.00025\n",
      "7 \t 0.1700 \t 2.2972\n",
      "8 \t 0.1700 \t 2.2972\n",
      "9 \t 0.1700 \t 2.2972\n",
      "10 \t 0.1700 \t 2.2972\n",
      "\n",
      "normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:109: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\JP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: RuntimeWarning: invalid value encountered in greater\n",
      "C:\\Users\\JP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:80: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0.1700 \t 2.2905\n",
      "2 \t 0.1700 \t 2.2951\n",
      "0.0005\n",
      "3 \t 0.1700 \t 2.2967\n",
      "4 \t 0.1700 \t 2.2971\n",
      "5 \t 0.1700 \t 2.2971\n",
      "6 \t 0.1700 \t 2.2972\n",
      "0.00025\n",
      "7 \t 0.1700 \t 2.2972\n",
      "8 \t 0.1700 \t 2.2972\n",
      "9 \t 0.1700 \t 2.2972\n",
      "10 \t 0.1700 \t 2.2972\n",
      "\n",
      "glorot\n",
      "1 \t 0.9100 \t 0.2143\n",
      "2 \t 0.9600 \t 0.1354\n",
      "0.0005\n",
      "3 \t 0.9600 \t 0.0950\n",
      "4 \t 0.9600 \t 0.0709\n",
      "5 \t 0.9700 \t 0.0554\n",
      "6 \t 0.9800 \t 0.0450\n",
      "0.00025\n",
      "7 \t 0.9900 \t 0.0375\n",
      "8 \t 0.9900 \t 0.0320\n",
      "9 \t 1.0000 \t 0.0277\n",
      "10 \t 1.0000 \t 0.0242\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for weight_mode in [\"zero\", \"normal\", \"glorot\"]:\n",
    "#for weight_mode in [\"glorot\"]:\n",
    "    print(weight_mode)\n",
    "    nn = NN(hidden_dims=(800,300),weight_mode=weight_mode, mode=\"ReLU\")\n",
    "    train_info = nn.train(X_train, y_train, epochs = 10, mini_batch_size=100, learning_rate = 0.001, lr_decay = 0.5)\n",
    "    plt.plot(np.arange(1,11,1), train_info[\"average_losses\"], label=weight_mode)\n",
    "    print()\n",
    "\n",
    "plt.title(\"Average loss by epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0.8750 \t 0.0321\n",
      "2 \t 0.9375 \t 0.0047\n",
      "3 \t 0.9375 \t 0.0020\n",
      "4 \t 1.0000 \t 0.0040\n",
      "0.0068000000000000005\n",
      "5 \t 1.0000 \t 0.0038\n",
      "6 \t 1.0000 \t 0.0027\n",
      "7 \t 1.0000 \t 0.0016\n",
      "8 \t 1.0000 \t 0.0008\n",
      "9 \t 1.0000 \t 0.0017\n",
      "10 \t 1.0000 \t 0.0016\n",
      "0.00544\n",
      "11 \t 1.0000 \t 0.0021\n",
      "12 \t 1.0000 \t 0.0002\n",
      "13 \t 1.0000 \t 0.0008\n",
      "14 \t 1.0000 \t 0.0006\n",
      "15 \t 1.0000 \t 0.0002\n",
      "16 \t 1.0000 \t 0.0005\n",
      "0.004352000000000001\n",
      "17 \t 1.0000 \t 0.0005\n",
      "18 \t 1.0000 \t 0.0001\n",
      "19 \t 1.0000 \t 0.0000\n",
      "20 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8750 \t 0.0321\n",
      "2 \t 0.9375 \t 0.0047\n",
      "3 \t 0.9375 \t 0.0020\n",
      "4 \t 1.0000 \t 0.0040\n",
      "5 \t 1.0000 \t 0.0038\n",
      "6 \t 1.0000 \t 0.0027\n",
      "7 \t 1.0000 \t 0.0016\n",
      "8 \t 1.0000 \t 0.0008\n",
      "9 \t 1.0000 \t 0.0017\n",
      "0.0068000000000000005\n",
      "10 \t 1.0000 \t 0.0016\n",
      "11 \t 1.0000 \t 0.0021\n",
      "12 \t 1.0000 \t 0.0002\n",
      "13 \t 1.0000 \t 0.0008\n",
      "14 \t 1.0000 \t 0.0006\n",
      "15 \t 1.0000 \t 0.0002\n",
      "16 \t 1.0000 \t 0.0005\n",
      "17 \t 1.0000 \t 0.0005\n",
      "18 \t 1.0000 \t 0.0001\n",
      "1 \t 0.8750 \t 0.0321\n",
      "2 \t 0.9375 \t 0.0047\n",
      "3 \t 0.9375 \t 0.0020\n",
      "4 \t 1.0000 \t 0.0040\n",
      "5 \t 1.0000 \t 0.0038\n",
      "6 \t 1.0000 \t 0.0027\n",
      "7 \t 1.0000 \t 0.0016\n",
      "8 \t 1.0000 \t 0.0008\n",
      "9 \t 1.0000 \t 0.0017\n",
      "10 \t 1.0000 \t 0.0016\n",
      "11 \t 1.0000 \t 0.0021\n",
      "0.0068000000000000005\n",
      "12 \t 1.0000 \t 0.0002\n",
      "13 \t 1.0000 \t 0.0008\n",
      "14 \t 1.0000 \t 0.0006\n",
      "15 \t 1.0000 \t 0.0002\n",
      "16 \t 1.0000 \t 0.0005\n",
      "17 \t 1.0000 \t 0.0005\n",
      "18 \t 1.0000 \t 0.0001\n",
      "19 \t 1.0000 \t 0.0000\n",
      "20 \t 1.0000 \t 0.0000\n",
      "21 \t 1.0000 \t 0.0000\n",
      "22 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8125 \t 0.0285\n",
      "2 \t 1.0000 \t 0.0108\n",
      "3 \t 1.0000 \t 0.0057\n",
      "4 \t 0.9375 \t 0.0017\n",
      "0.0064\n",
      "5 \t 1.0000 \t 0.0032\n",
      "6 \t 1.0000 \t 0.0023\n",
      "7 \t 1.0000 \t 0.0031\n",
      "8 \t 1.0000 \t 0.0006\n",
      "9 \t 1.0000 \t 0.0002\n",
      "10 \t 1.0000 \t 0.0004\n",
      "0.00512\n",
      "11 \t 1.0000 \t 0.0007\n",
      "12 \t 1.0000 \t 0.0011\n",
      "13 \t 1.0000 \t 0.0002\n",
      "14 \t 1.0000 \t 0.0001\n",
      "15 \t 1.0000 \t 0.0011\n",
      "16 \t 1.0000 \t 0.0005\n",
      "0.004096000000000001\n",
      "17 \t 1.0000 \t 0.0000\n",
      "18 \t 1.0000 \t 0.0000\n",
      "19 \t 1.0000 \t 0.0000\n",
      "20 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8125 \t 0.0285\n",
      "2 \t 1.0000 \t 0.0108\n",
      "3 \t 1.0000 \t 0.0057\n",
      "4 \t 0.9375 \t 0.0017\n",
      "5 \t 1.0000 \t 0.0032\n",
      "6 \t 1.0000 \t 0.0023\n",
      "7 \t 1.0000 \t 0.0031\n",
      "8 \t 1.0000 \t 0.0006\n",
      "9 \t 1.0000 \t 0.0002\n",
      "0.0064\n",
      "10 \t 1.0000 \t 0.0004\n",
      "11 \t 1.0000 \t 0.0007\n",
      "12 \t 1.0000 \t 0.0011\n",
      "13 \t 1.0000 \t 0.0002\n",
      "14 \t 1.0000 \t 0.0001\n",
      "15 \t 1.0000 \t 0.0011\n",
      "16 \t 1.0000 \t 0.0005\n",
      "17 \t 1.0000 \t 0.0000\n",
      "18 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8125 \t 0.0285\n",
      "2 \t 1.0000 \t 0.0108\n",
      "3 \t 1.0000 \t 0.0057\n",
      "4 \t 0.9375 \t 0.0017\n",
      "5 \t 1.0000 \t 0.0032\n",
      "6 \t 1.0000 \t 0.0023\n",
      "7 \t 1.0000 \t 0.0031\n",
      "8 \t 1.0000 \t 0.0006\n",
      "9 \t 1.0000 \t 0.0002\n",
      "10 \t 1.0000 \t 0.0004\n",
      "11 \t 1.0000 \t 0.0007\n",
      "0.0064\n",
      "12 \t 1.0000 \t 0.0011\n",
      "13 \t 1.0000 \t 0.0002\n",
      "14 \t 1.0000 \t 0.0001\n",
      "15 \t 1.0000 \t 0.0011\n",
      "16 \t 1.0000 \t 0.0005\n",
      "17 \t 1.0000 \t 0.0000\n",
      "18 \t 1.0000 \t 0.0000\n",
      "19 \t 1.0000 \t 0.0000\n",
      "20 \t 1.0000 \t 0.0000\n",
      "21 \t 1.0000 \t 0.0000\n",
      "22 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8750 \t 0.0254\n",
      "2 \t 0.9375 \t 0.0084\n",
      "3 \t 1.0000 \t 0.0030\n",
      "4 \t 1.0000 \t 0.0027\n",
      "0.006\n",
      "5 \t 1.0000 \t 0.0026\n",
      "6 \t 1.0000 \t 0.0013\n",
      "7 \t 1.0000 \t 0.0011\n",
      "8 \t 1.0000 \t 0.0012\n",
      "9 \t 1.0000 \t 0.0014\n",
      "10 \t 1.0000 \t 0.0005\n",
      "0.0048000000000000004\n",
      "11 \t 1.0000 \t 0.0012\n",
      "12 \t 1.0000 \t 0.0004\n",
      "13 \t 1.0000 \t 0.0002\n",
      "14 \t 1.0000 \t 0.0000\n",
      "15 \t 1.0000 \t 0.0000\n",
      "16 \t 1.0000 \t 0.0000\n",
      "0.0038400000000000005\n",
      "17 \t 1.0000 \t 0.0000\n",
      "18 \t 1.0000 \t 0.0000\n",
      "19 \t 1.0000 \t 0.0000\n",
      "20 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8750 \t 0.0254\n",
      "2 \t 0.9375 \t 0.0084\n",
      "3 \t 1.0000 \t 0.0030\n",
      "4 \t 1.0000 \t 0.0027\n",
      "5 \t 1.0000 \t 0.0026\n",
      "6 \t 1.0000 \t 0.0013\n",
      "7 \t 1.0000 \t 0.0011\n",
      "8 \t 1.0000 \t 0.0012\n",
      "9 \t 1.0000 \t 0.0014\n",
      "0.006\n",
      "10 \t 1.0000 \t 0.0005\n",
      "11 \t 1.0000 \t 0.0012\n",
      "12 \t 1.0000 \t 0.0004\n",
      "13 \t 1.0000 \t 0.0002\n",
      "14 \t 1.0000 \t 0.0000\n",
      "15 \t 1.0000 \t 0.0000\n",
      "16 \t 1.0000 \t 0.0000\n",
      "17 \t 1.0000 \t 0.0000\n",
      "18 \t 1.0000 \t 0.0000\n",
      "1 \t 0.8750 \t 0.0254\n",
      "2 \t 0.9375 \t 0.0084\n",
      "3 \t 1.0000 \t 0.0030\n",
      "4 \t 1.0000 \t 0.0027\n",
      "5 \t 1.0000 \t 0.0026\n",
      "6 \t 1.0000 \t 0.0013\n",
      "7 \t 1.0000 \t 0.0011\n",
      "8 \t 1.0000 \t 0.0012\n",
      "9 \t 1.0000 \t 0.0014\n",
      "10 \t 1.0000 \t 0.0005\n",
      "11 \t 1.0000 \t 0.0012\n",
      "0.006\n",
      "12 \t 1.0000 \t 0.0004\n",
      "13 \t 1.0000 \t 0.0002\n",
      "14 \t 1.0000 \t 0.0000\n",
      "15 \t 1.0000 \t 0.0000\n",
      "16 \t 1.0000 \t 0.0000\n",
      "17 \t 1.0000 \t 0.0000\n",
      "18 \t 1.0000 \t 0.0000\n",
      "19 \t 1.0000 \t 0.0000\n",
      "20 \t 1.0000 \t 0.0000\n",
      "21 \t 1.0000 \t 0.0000\n",
      "22 \t 1.0000 \t 0.0000\n",
      "0.9854 {'lr': 0.0075, 'hd': (625, 625), 'mb': 64, 'epoch': 22}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "\n",
    "lr = [0.0085, 0.008, 0.0075]\n",
    "hiddens_dims = [(625,625)]\n",
    "mini_batch = [64]\n",
    "epoch = (20,18,22)\n",
    "\n",
    "\n",
    "# Mode = ReLU\n",
    "best = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr_ in lr:\n",
    "    for hd in hiddens_dims:\n",
    "        for mb in mini_batch:\n",
    "            for epoch_ in epoch:\n",
    "                #print(\"Lr : {}, hd : {}, mb : {}\".format(lr_, hd, mb))\n",
    "                model = NN(hidden_dims = hd, weight_mode = \"glorot\", mode = \"ReLU\")\n",
    "                train_log = model.train(X_train, y_train, X_valid, y_valid, epochs = epoch_,\n",
    "                                        mini_batch_size=mb, learning_rate = lr_, lr_decay = 0.8,\n",
    "                                        lr_decay_intervals = [0.2, 0.5, 0.8])\n",
    "                acc = model.test(X_valid, y_valid)\n",
    "                if(acc>best):\n",
    "                    best = acc\n",
    "                    best_params[\"lr\"] = lr_\n",
    "                    best_params[\"hd\"] = hd\n",
    "                    best_params[\"mb\"] = mb\n",
    "                    best_params[\"epoch\"]=epoch_\n",
    "                              \n",
    "print(best, best_params)\n",
    "\n",
    "# # Mode = sigmoid\n",
    "# best_s = 0\n",
    "# best_params_s = {}\n",
    "\n",
    "# for lr_ in lr:\n",
    "#     for hd in hiddens_dims:\n",
    "#         for mb in mini_batch:\n",
    "#             for epoch_ in epoch:\n",
    "#                 #print(\"Lr : {}, hd : {}, mb : {}\".format(lr_, hd, mb))\n",
    "#                 model = NN(hidden_dims = hd, weight_mode = \"glorot\", mode = \"sigmoid\")\n",
    "#                 train_log = model.train(X_train, y_train, epochs = epoch_, mini_batch_size=mb, learning_rate = lr_)\n",
    "#                 acc = model.test(X_valid, y_valid)\n",
    "#                 if(acc>best_s):\n",
    "#                     best_s = acc\n",
    "#                     best_params_s[\"lr\"] = lr_\n",
    "#                     best_params_s[\"hd\"] = hd\n",
    "#                     best_params_s[\"mb\"] = mb\n",
    "#                     best_params_s[\"epoch\"]=epoch_\n",
    "                              \n",
    "# print(best_s, best_params_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96.3% lr = 0.008, hd = 625,625, mb = 64, epoch = 40\n",
    "\n",
    "96.8% lr = 0.008, hd = 625,625, mb = 64, epoch = 20\n",
    "\n",
    "98.5% lr = 0.0075, hd = 625, 625, mb = 64, epoch = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Gradients Using Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from Marc-Andr√©'s homework#3 of IFT6390 class.\n",
    "lr = 0.0075\n",
    "hd = (625,625)\n",
    "mb = 64\n",
    "epoch = 10\n",
    "eps = (1/10, 1/50, 1/100, 1/500, 1/1000)\n",
    "\n",
    "model_finite_diff = NN(hidden_dims=hd, weight_mode=\"glorot\", mode=\"ReLU\")\n",
    "log = model_finite_diff.train(X_train, y_train, X_valid, y_valid, epochs = epoch,\n",
    "                                        mini_batch_size=mb, learning_rate = lr, lr_decay = 0.8,\n",
    "                                        lr_decay_intervals = [0.2, 0.5, 0.8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "-0.562742420250302\n",
      "0.0006869081364070681\n",
      "-0.7627424202503019\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7427424202503019\n",
      "0.0006869081364070681\n",
      "-0.7827424202503019\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.7727424202503019\n",
      "0.0006869081364070681\n",
      "-0.792742420250302\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.790742420250302\n",
      "0.0006869081364070681\n",
      "-0.794742420250302\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.793742420250302\n",
      "0.0006869081364070681\n",
      "-0.795742420250302\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.5066660597218868\n",
      "0.0006869081364070681\n",
      "-0.7066660597218868\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.6866660597218868\n",
      "0.0006869081364070681\n",
      "-0.7266660597218868\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.7166660597218868\n",
      "0.0006869081364070681\n",
      "-0.7366660597218868\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.7346660597218868\n",
      "0.0006869081364070681\n",
      "-0.7386660597218868\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.7376660597218868\n",
      "0.0006869081364070681\n",
      "-0.7396660597218868\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.5303543414622729\n",
      "0.0006869081364070681\n",
      "-0.7303543414622728\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7103543414622728\n",
      "0.0006869081364070681\n",
      "-0.7503543414622729\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.7403543414622729\n",
      "0.0006869081364070681\n",
      "-0.7603543414622729\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.7583543414622729\n",
      "0.0006869081364070681\n",
      "-0.7623543414622729\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.7613543414622729\n",
      "0.0006869081364070681\n",
      "-0.7633543414622729\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.593399932643025\n",
      "0.0006869081364070681\n",
      "-0.793399932643025\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.773399932643025\n",
      "0.0006869081364070681\n",
      "-0.813399932643025\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.803399932643025\n",
      "0.0006869081364070681\n",
      "-0.823399932643025\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.821399932643025\n",
      "0.0006869081364070681\n",
      "-0.825399932643025\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.824399932643025\n",
      "0.0006869081364070681\n",
      "-0.826399932643025\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.6014069006688973\n",
      "0.0006869081364070681\n",
      "-0.8014069006688973\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7814069006688973\n",
      "0.0006869081364070681\n",
      "-0.8214069006688973\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.8114069006688973\n",
      "0.0006869081364070681\n",
      "-0.8314069006688973\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.8294069006688973\n",
      "0.0006869081364070681\n",
      "-0.8334069006688973\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.8324069006688973\n",
      "0.0006869081364070681\n",
      "-0.8344069006688973\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.540695801156535\n",
      "0.0006869081364070681\n",
      "-0.740695801156535\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.720695801156535\n",
      "0.0006869081364070681\n",
      "-0.760695801156535\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.750695801156535\n",
      "0.0006869081364070681\n",
      "-0.770695801156535\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.768695801156535\n",
      "0.0006869081364070681\n",
      "-0.772695801156535\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.771695801156535\n",
      "0.0006869081364070681\n",
      "-0.773695801156535\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.6084422962493112\n",
      "0.0006869081364070681\n",
      "-0.8084422962493112\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7884422962493112\n",
      "0.0006869081364070681\n",
      "-0.8284422962493112\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.8184422962493112\n",
      "0.0006869081364070681\n",
      "-0.8384422962493112\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.8364422962493112\n",
      "0.0006869081364070681\n",
      "-0.8404422962493112\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.8394422962493112\n",
      "0.0006869081364070681\n",
      "-0.8414422962493112\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.5790375384520754\n",
      "0.0006869081364070681\n",
      "-0.7790375384520754\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7590375384520753\n",
      "0.0006869081364070681\n",
      "-0.7990375384520754\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.7890375384520754\n",
      "0.0006869081364070681\n",
      "-0.8090375384520754\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.8070375384520754\n",
      "0.0006869081364070681\n",
      "-0.8110375384520754\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.8100375384520754\n",
      "0.0006869081364070681\n",
      "-0.8120375384520754\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.5495927634313014\n",
      "0.0006869081364070681\n",
      "-0.7495927634313013\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7295927634313013\n",
      "0.0006869081364070681\n",
      "-0.7695927634313013\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.7595927634313013\n",
      "0.0006869081364070681\n",
      "-0.7795927634313013\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.7775927634313013\n",
      "0.0006869081364070681\n",
      "-0.7815927634313014\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.7805927634313014\n",
      "0.0006869081364070681\n",
      "-0.7825927634313014\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.1\n",
      "-0.5764941709730547\n",
      "0.0006869081364070681\n",
      "-0.7764941709730546\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.02\n",
      "-0.7564941709730546\n",
      "0.0006869081364070681\n",
      "-0.7964941709730546\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.01\n",
      "-0.7864941709730546\n",
      "0.0006869081364070681\n",
      "-0.8064941709730546\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.002\n",
      "-0.8044941709730546\n",
      "0.0006869081364070681\n",
      "-0.8084941709730546\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n",
      "0.001\n",
      "-0.8074941709730546\n",
      "0.0006869081364070681\n",
      "-0.8094941709730546\n",
      "0.0006869081364070681\n",
      "Finite Gradient :  0.0\n",
      "Backprop gradient :  0.0\n",
      "Ratio :  nan\n",
      "----------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def finite_diff_gradient_check(model, sample_X, sample_Y, weights_idx = 10, eps = (1/100)):\n",
    "    \"\"\"\n",
    "    -----\n",
    "    Parameters: \n",
    "        model: neural net with initialized weights\n",
    "        weights_idx: first weights to evaluate theta = 1:weights_idx\n",
    "        eps: epsilon values to try for finite difference\n",
    "        sample_X : data point\n",
    "        sample_Y : data point label\n",
    "    \"\"\"\n",
    "    for i in range(weights_idx):\n",
    "        for eps_ in eps:\n",
    "            print(eps_)\n",
    "            model.W1[0,i] += eps_\n",
    "            print(model.W1[0,i])\n",
    "            loss_eps_plus = model.loss(sample_X, sample_Y)\n",
    "            print(loss_eps_plus)\n",
    "            model.W1[0,i] -= 2*eps_\n",
    "            print(model.W1[0,i])\n",
    "            loss_eps_minus = model.loss(sample_X, sample_Y)\n",
    "            model.W1[0,i] += eps_\n",
    "            print(loss_eps_minus)\n",
    "            finite_diff = (loss_eps_plus - loss_eps_minus)/(2*eps_)\n",
    "            print(\"Finite Gradient : \",finite_diff)\n",
    "            print(\"Backprop gradient : \", model.grads[\"W1\"][0,i])\n",
    "            print(\"Ratio : \", finite_diff / model.grads[\"W1\"][0,i])\n",
    "            print(\"-----------------------------------\", '\\n')\n",
    "\n",
    "            \n",
    "\n",
    "finite_diff_gradient_check(model_finite_diff, (X_train[0, np.newaxis]), y_train[0], eps=eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_Y = to_categorical(Y, num_classes=self.m).reshape((1, self.m))\n",
    "self.bprop(X,onehot_Y)\n",
    "\n",
    "loss_fprop = self.loss\n",
    "\n",
    "eta = 10**(-5)\n",
    "\n",
    "finite_grad_W1 = self.check_onebyone(X, Y, eta, loss_fprop, self.W1, (self.dh, self.d))\n",
    "ratio_W1 = np.nanmean(finite_grad_W1/self.grad_W1)\n",
    "\n",
    "finite_grad_W2 = self.check_onebyone(X, Y, eta, loss_fprop, self.W2, (self.m, self.dh))\n",
    "ratio_W2 = np.nanmean(finite_grad_W2/self.grad_W2)\n",
    "\n",
    "finite_grad_b1 = self.check_onebyone(X, Y, eta, loss_fprop, self.b1, (1, self.dh))\n",
    "ratio_b1 = np.nanmean(finite_grad_b1/self.grad_b1.reshape((1, self.dh)))\n",
    "\n",
    "finite_grad_b2 = self.check_onebyone(X, Y, eta, loss_fprop, self.b2, (1, self.m))\n",
    "ratio_b2 = np.nanmean(finite_grad_b2/self.grad_b2.reshape((1, self.m)))\n",
    "\n",
    "check = True\n",
    "ratios = [ratio_W1 ,ratio_W2, ratio_b1, ratio_b2]\n",
    "\n",
    "for ratio in ratios:\n",
    "    check = check and ratio < 1.01 and ratio > 0.99\n",
    "\n",
    "if (verbose):\n",
    "    print(\"Backprop gradient W1\")\n",
    "    print(np.nan_to_num(self.grad_W1))\n",
    "    print(\"Finite gradient W1\")\n",
    "    print(np.nan_to_num(finite_grad_W1))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient b1\")\n",
    "    print(np.nan_to_num(self.grad_b1))\n",
    "    print(\"Finite gradient b1\")\n",
    "    print(np.nan_to_num(finite_grad_b1))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient W2\")\n",
    "    print(np.nan_to_num(self.grad_W2))\n",
    "    print(\"Finite gradient W2\")\n",
    "    print(np.nan_to_num(finite_grad_W2))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(\"Backprop gradient b2\")\n",
    "    print(np.nan_to_num(self.grad_b2))\n",
    "    print(\"Finite gradient b2\")\n",
    "    print(np.nan_to_num(finite_grad_b2))\n",
    "    print(\"     -------------------------------\")\n",
    "    print(f\" ratio W1 : {ratio_W1}  \\n ratio b1 : {ratio_b1} \\\n",
    "          \\n ratio W2 : {ratio_W2} \\n ratio b2 : {ratio_b2}\")\n",
    "\n",
    "return check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
