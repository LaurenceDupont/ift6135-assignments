{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as DS\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# If a GPU is available, use it\n",
    "# Pytorch uses an elegant way to keep the code device agnostic\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "    \n",
    "print(device)\n",
    "\n",
    "\n",
    "# If in clolab environment or not\n",
    "isColab = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isColab:\n",
    "    trainImageFolder = \"./trainset/\"\n",
    "    testImageFolder = \"./testset/\"\n",
    "else:\n",
    "    # code here to fetch and load colab folders.\n",
    "    noop()\n",
    "    \n",
    "BATCH_SIZE=512 \n",
    "\n",
    "# Data augmentation is done through the Randomization of the transforms applied each time an image is loaded in the epoche.\n",
    "# Each epoche will have different subset of transformations done on the images.\n",
    "Transform_img = transforms.Compose([\n",
    "    transforms.RandomRotation((10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "tensor_img_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load from image folder directly, and assign the data augmentaion through the transform\n",
    "full_dataset = DS.ImageFolder(root=trainImageFolder, transform=Transform_img)\n",
    "\n",
    "# Seperate the training and validation set: 80% training, 20% validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "valid_size = len(full_dataset) - train_size\n",
    "train_data, valid_data = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,  \n",
    "    num_workers=4,\n",
    "    pin_memory=use_cuda\n",
    ")\n",
    "\n",
    "valid_loader = data.DataLoader(\n",
    "    valid_data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,  \n",
    "    num_workers=4,\n",
    "    pin_memory=use_cuda\n",
    ")\n",
    "\n",
    "indices = list(range(len(train_data)))\n",
    "random.shuffle(indices)\n",
    "scratch_loader = data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=SubsetRandomSampler(indices[:64]),\n",
    "    num_workers=1,\n",
    "    pin_memory=use_cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape of one batch of inputs torch.Size([512, 3, 64, 64])\n",
      "This is the shape of one batch of targets torch.Size([512])\n",
      "This is an image of class tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step, (x, y) in enumerate(train_loader):\n",
    "    \n",
    "    print(f\"This is the shape of one batch of inputs {x.shape}\")\n",
    "    print(f\"This is the shape of one batch of targets {y.shape}\")\n",
    "    print(\"This is an image of class\", y[0])\n",
    "\n",
    "    img = np.transpose(x[step].numpy(), (1, 2, 0)) # We actually want the chanels in the last column\n",
    "    plt.imshow(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions\n",
    "Heavely inspired from https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/ (MIT Liscence)\n",
    "The BasicBlock is extracted from the repositery, but it's the building block for a resnet with bypass connections. The normalisations were removed since we were not allowed to use them whithout coding them ourselfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        #self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        #self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.downsample = None\n",
    "        if inplanes != planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                conv1x1(inplanes, planes, stride),\n",
    "                # nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        # out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        # out = self.bn2(out)\n",
    "        \n",
    "        # Reduce the sample size so the residual matches size\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    \"\"\"Affordable convolutions for the people.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(3, 64, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = BasicBlock(64, 64)\n",
    "        self.layer2= BasicBlock(64, 128, 2)\n",
    "        self.layer3= BasicBlock(128, 128)\n",
    "        self.layer4= BasicBlock(128, 256, 2)\n",
    "        self.layer5= BasicBlock(256, 256)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(256, 2)\n",
    "        self.sm = nn.Softmax() # Because we are doing classification. We could use Sigmoid since this is a binary classification, \n",
    "\n",
    "    def forward(self, xin):\n",
    "        x = self.conv1(xin)\n",
    "       # x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.sm(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate loss used for training\n",
    "# If we were to use Softmax as our activation in our model, we should actually use Binary Cross Entropy Loss\n",
    "# nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# spot to save your learning curves, and potentially checkpoint your models\n",
    "savedir = 'results'\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "## This method comes from the lab session number 11 of the class\n",
    "def train(model, train_loader, optimizer, epoch ):\n",
    "    \"\"\"Perform one epoch of training.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs, target = inputs.to(device), target.to(device) # Send data to the device\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This method comes from the lab session number 11 of the class\n",
    "def test(model, test_loader):\n",
    "    \"\"\"Evaluate the model by doing one pass over a dataset\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "    \n",
    "    with torch.no_grad(): # save some computations\n",
    "      \n",
    "        for inputs, target in test_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            test_size += target.size(0)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = test_loss_fn(output, target)\n",
    "            test_loss += loss\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]# [1] return s the arg max. i.e.  si idx == target, get the index of the prediction\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, test_size,\n",
    "        100. * accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting capacity \n",
    "We want to validate we have the capacity to at least overfit a small number of examples to make sure our model is at least learning something. This also helps to determin the step size we should use for the longer learning cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/64 (0%)]\tLoss: 0.691963\n",
      "Test set: Average loss: 0.6919, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 2 [0/64 (0%)]\tLoss: 0.691853\n",
      "Test set: Average loss: 0.6918, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/64 (0%)]\tLoss: 0.691753\n",
      "Test set: Average loss: 0.6917, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 4 [0/64 (0%)]\tLoss: 0.691662\n",
      "Test set: Average loss: 0.6916, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 5 [0/64 (0%)]\tLoss: 0.691578\n",
      "Test set: Average loss: 0.6915, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/64 (0%)]\tLoss: 0.691500\n",
      "Test set: Average loss: 0.6914, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 7 [0/64 (0%)]\tLoss: 0.691428\n",
      "Test set: Average loss: 0.6914, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 8 [0/64 (0%)]\tLoss: 0.691362\n",
      "Test set: Average loss: 0.6913, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 9 [0/64 (0%)]\tLoss: 0.691301\n",
      "Test set: Average loss: 0.6912, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 10 [0/64 (0%)]\tLoss: 0.691244\n",
      "Test set: Average loss: 0.6912, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 11 [0/64 (0%)]\tLoss: 0.691190\n",
      "Test set: Average loss: 0.6911, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 12 [0/64 (0%)]\tLoss: 0.691139\n",
      "Test set: Average loss: 0.6911, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 13 [0/64 (0%)]\tLoss: 0.691092\n",
      "Test set: Average loss: 0.6910, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 14 [0/64 (0%)]\tLoss: 0.691046\n",
      "Test set: Average loss: 0.6910, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 15 [0/64 (0%)]\tLoss: 0.691003\n",
      "Test set: Average loss: 0.6910, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 16 [0/64 (0%)]\tLoss: 0.690962\n",
      "Test set: Average loss: 0.6909, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 17 [0/64 (0%)]\tLoss: 0.690923\n",
      "Test set: Average loss: 0.6909, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 18 [0/64 (0%)]\tLoss: 0.690886\n",
      "Test set: Average loss: 0.6909, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 19 [0/64 (0%)]\tLoss: 0.690852\n",
      "Test set: Average loss: 0.6908, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 20 [0/64 (0%)]\tLoss: 0.690819\n",
      "Test set: Average loss: 0.6908, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 21 [0/64 (0%)]\tLoss: 0.690788\n",
      "Test set: Average loss: 0.6908, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 22 [0/64 (0%)]\tLoss: 0.690758\n",
      "Test set: Average loss: 0.6907, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 23 [0/64 (0%)]\tLoss: 0.690730\n",
      "Test set: Average loss: 0.6907, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 24 [0/64 (0%)]\tLoss: 0.690702\n",
      "Test set: Average loss: 0.6907, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 25 [0/64 (0%)]\tLoss: 0.690675\n",
      "Test set: Average loss: 0.6906, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 26 [0/64 (0%)]\tLoss: 0.690649\n",
      "Test set: Average loss: 0.6906, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 27 [0/64 (0%)]\tLoss: 0.690623\n",
      "Test set: Average loss: 0.6906, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 28 [0/64 (0%)]\tLoss: 0.690598\n",
      "Test set: Average loss: 0.6906, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 29 [0/64 (0%)]\tLoss: 0.690574\n",
      "Test set: Average loss: 0.6906, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 30 [0/64 (0%)]\tLoss: 0.690551\n",
      "Test set: Average loss: 0.6905, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 31 [0/64 (0%)]\tLoss: 0.690528\n",
      "Test set: Average loss: 0.6905, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 32 [0/64 (0%)]\tLoss: 0.690506\n",
      "Test set: Average loss: 0.6905, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 33 [0/64 (0%)]\tLoss: 0.690484\n",
      "Test set: Average loss: 0.6905, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 34 [0/64 (0%)]\tLoss: 0.690462\n",
      "Test set: Average loss: 0.6904, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 35 [0/64 (0%)]\tLoss: 0.690439\n",
      "Test set: Average loss: 0.6904, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 36 [0/64 (0%)]\tLoss: 0.690417\n",
      "Test set: Average loss: 0.6904, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 37 [0/64 (0%)]\tLoss: 0.690396\n",
      "Test set: Average loss: 0.6904, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 38 [0/64 (0%)]\tLoss: 0.690374\n",
      "Test set: Average loss: 0.6904, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 39 [0/64 (0%)]\tLoss: 0.690353\n",
      "Test set: Average loss: 0.6903, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 40 [0/64 (0%)]\tLoss: 0.690331\n",
      "Test set: Average loss: 0.6903, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 41 [0/64 (0%)]\tLoss: 0.690309\n",
      "Test set: Average loss: 0.6903, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 42 [0/64 (0%)]\tLoss: 0.690288\n",
      "Test set: Average loss: 0.6903, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 43 [0/64 (0%)]\tLoss: 0.690266\n",
      "Test set: Average loss: 0.6902, Accuracy: 34/64 (53%)\n",
      "\n",
      "Train Epoch: 44 [0/64 (0%)]\tLoss: 0.690244\n",
      "Test set: Average loss: 0.6902, Accuracy: 34/64 (53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overfit_model = BasicNet().to(device)\n",
    "\n",
    "lr = 0.05\n",
    "optimizer = optim.SGD(overfit_model.parameters(), weight_decay=1e-6, lr=lr)\n",
    "\n",
    "#results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
    "#savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
    "\n",
    "for epoch in range(1, 45):\n",
    "    train(overfit_model, scratch_loader, optimizer, epoch)\n",
    "    loss, acc = test(overfit_model, scratch_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/16384 (0%)]\tLoss: 0.693509\n",
      "Train Epoch: 1 [5120/16384 (31%)]\tLoss: 0.693145\n",
      "Train Epoch: 1 [10240/16384 (62%)]\tLoss: 0.693136\n",
      "Train Epoch: 1 [15360/16384 (94%)]\tLoss: 0.693067\n",
      "Test set: Average loss: 0.6930, Accuracy: 2028/4000 (51%)\n",
      "\n",
      "Best acc:  0.507\n",
      "Train Epoch: 2 [0/16384 (0%)]\tLoss: 0.693041\n",
      "Train Epoch: 2 [5120/16384 (31%)]\tLoss: 0.693002\n",
      "Train Epoch: 2 [10240/16384 (62%)]\tLoss: 0.693010\n",
      "Train Epoch: 2 [15360/16384 (94%)]\tLoss: 0.693099\n",
      "Test set: Average loss: 0.6929, Accuracy: 2028/4000 (51%)\n",
      "\n",
      "Train Epoch: 3 [0/16384 (0%)]\tLoss: 0.692908\n",
      "Train Epoch: 3 [5120/16384 (31%)]\tLoss: 0.693098\n",
      "Train Epoch: 3 [10240/16384 (62%)]\tLoss: 0.692852\n",
      "Train Epoch: 3 [15360/16384 (94%)]\tLoss: 0.692955\n",
      "Test set: Average loss: 0.6927, Accuracy: 2030/4000 (51%)\n",
      "\n",
      "Best acc:  0.5075\n",
      "Train Epoch: 4 [0/16384 (0%)]\tLoss: 0.692746\n",
      "Train Epoch: 4 [5120/16384 (31%)]\tLoss: 0.692489\n",
      "Train Epoch: 4 [10240/16384 (62%)]\tLoss: 0.693239\n",
      "Train Epoch: 4 [15360/16384 (94%)]\tLoss: 0.692778\n",
      "Test set: Average loss: 0.6926, Accuracy: 2365/4000 (59%)\n",
      "\n",
      "Best acc:  0.59125\n",
      "Train Epoch: 5 [0/16384 (0%)]\tLoss: 0.692680\n",
      "Train Epoch: 5 [5120/16384 (31%)]\tLoss: 0.692745\n",
      "Train Epoch: 5 [10240/16384 (62%)]\tLoss: 0.693073\n",
      "Train Epoch: 5 [15360/16384 (94%)]\tLoss: 0.692389\n",
      "Test set: Average loss: 0.6924, Accuracy: 2397/4000 (60%)\n",
      "\n",
      "Best acc:  0.59925\n",
      "Train Epoch: 6 [0/16384 (0%)]\tLoss: 0.692697\n",
      "Train Epoch: 6 [5120/16384 (31%)]\tLoss: 0.692298\n",
      "Train Epoch: 6 [10240/16384 (62%)]\tLoss: 0.692017\n",
      "Train Epoch: 6 [15360/16384 (94%)]\tLoss: 0.692468\n",
      "Test set: Average loss: 0.6921, Accuracy: 2220/4000 (56%)\n",
      "\n",
      "Train Epoch: 7 [0/16384 (0%)]\tLoss: 0.692095\n",
      "Train Epoch: 7 [5120/16384 (31%)]\tLoss: 0.692444\n",
      "Train Epoch: 7 [10240/16384 (62%)]\tLoss: 0.693188\n",
      "Train Epoch: 7 [15360/16384 (94%)]\tLoss: 0.691496\n",
      "Test set: Average loss: 0.6915, Accuracy: 2035/4000 (51%)\n",
      "\n",
      "Train Epoch: 8 [0/16384 (0%)]\tLoss: 0.693130\n",
      "Train Epoch: 8 [5120/16384 (31%)]\tLoss: 0.691920\n",
      "Train Epoch: 8 [10240/16384 (62%)]\tLoss: 0.691522\n",
      "Train Epoch: 8 [15360/16384 (94%)]\tLoss: 0.691361\n",
      "Test set: Average loss: 0.6911, Accuracy: 2320/4000 (58%)\n",
      "\n",
      "Train Epoch: 9 [0/16384 (0%)]\tLoss: 0.691394\n",
      "Train Epoch: 9 [5120/16384 (31%)]\tLoss: 0.690678\n",
      "Train Epoch: 9 [10240/16384 (62%)]\tLoss: 0.690787\n",
      "Train Epoch: 9 [15360/16384 (94%)]\tLoss: 0.689855\n",
      "Test set: Average loss: 0.6914, Accuracy: 2028/4000 (51%)\n",
      "\n",
      "Train Epoch: 10 [0/16384 (0%)]\tLoss: 0.692970\n",
      "Train Epoch: 10 [5120/16384 (31%)]\tLoss: 0.689924\n",
      "Train Epoch: 10 [10240/16384 (62%)]\tLoss: 0.689943\n",
      "Train Epoch: 10 [15360/16384 (94%)]\tLoss: 0.690454\n",
      "Test set: Average loss: 0.6874, Accuracy: 2360/4000 (59%)\n",
      "\n",
      "Train Epoch: 11 [0/16384 (0%)]\tLoss: 0.686385\n",
      "Train Epoch: 11 [5120/16384 (31%)]\tLoss: 0.686456\n",
      "Train Epoch: 11 [10240/16384 (62%)]\tLoss: 0.687804\n",
      "Train Epoch: 11 [15360/16384 (94%)]\tLoss: 0.686045\n",
      "Test set: Average loss: 0.6862, Accuracy: 2246/4000 (56%)\n",
      "\n",
      "Train Epoch: 12 [0/16384 (0%)]\tLoss: 0.685445\n",
      "Train Epoch: 12 [5120/16384 (31%)]\tLoss: 0.684627\n",
      "Train Epoch: 12 [10240/16384 (62%)]\tLoss: 0.697740\n",
      "Train Epoch: 12 [15360/16384 (94%)]\tLoss: 0.688252\n",
      "Test set: Average loss: 0.7042, Accuracy: 1972/4000 (49%)\n",
      "\n",
      "Train Epoch: 13 [0/16384 (0%)]\tLoss: 0.697066\n",
      "Train Epoch: 13 [5120/16384 (31%)]\tLoss: 0.689409\n",
      "Train Epoch: 13 [10240/16384 (62%)]\tLoss: 0.684959\n",
      "Train Epoch: 13 [15360/16384 (94%)]\tLoss: 0.684198\n",
      "Test set: Average loss: 0.6829, Accuracy: 2377/4000 (59%)\n",
      "\n",
      "Train Epoch: 14 [0/16384 (0%)]\tLoss: 0.685217\n",
      "Train Epoch: 14 [5120/16384 (31%)]\tLoss: 0.696693\n",
      "Train Epoch: 14 [10240/16384 (62%)]\tLoss: 0.685708\n",
      "Train Epoch: 14 [15360/16384 (94%)]\tLoss: 0.699202\n",
      "Test set: Average loss: 0.7017, Accuracy: 1976/4000 (49%)\n",
      "\n",
      "Train Epoch: 15 [0/16384 (0%)]\tLoss: 0.692453\n",
      "Train Epoch: 15 [5120/16384 (31%)]\tLoss: 0.687492\n",
      "Train Epoch: 15 [10240/16384 (62%)]\tLoss: 0.682801\n",
      "Train Epoch: 15 [15360/16384 (94%)]\tLoss: 0.693535\n",
      "Test set: Average loss: 0.6789, Accuracy: 2285/4000 (57%)\n",
      "\n",
      "Train Epoch: 16 [0/16384 (0%)]\tLoss: 0.682657\n",
      "Train Epoch: 16 [5120/16384 (31%)]\tLoss: 0.690699\n",
      "Train Epoch: 16 [10240/16384 (62%)]\tLoss: 0.678020\n",
      "Train Epoch: 16 [15360/16384 (94%)]\tLoss: 0.678609\n",
      "Test set: Average loss: 0.6760, Accuracy: 2297/4000 (57%)\n",
      "\n",
      "Train Epoch: 17 [0/16384 (0%)]\tLoss: 0.678393\n",
      "Train Epoch: 17 [5120/16384 (31%)]\tLoss: 0.681324\n",
      "Train Epoch: 17 [10240/16384 (62%)]\tLoss: 0.669863\n",
      "Train Epoch: 17 [15360/16384 (94%)]\tLoss: 0.678846\n",
      "Test set: Average loss: 0.7023, Accuracy: 1988/4000 (50%)\n",
      "\n",
      "Train Epoch: 18 [0/16384 (0%)]\tLoss: 0.697786\n",
      "Train Epoch: 18 [5120/16384 (31%)]\tLoss: 0.675963\n",
      "Train Epoch: 18 [10240/16384 (62%)]\tLoss: 0.697039\n",
      "Train Epoch: 18 [15360/16384 (94%)]\tLoss: 0.715667\n",
      "Test set: Average loss: 0.6908, Accuracy: 2090/4000 (52%)\n",
      "\n",
      "Train Epoch: 19 [0/16384 (0%)]\tLoss: 0.688231\n",
      "Train Epoch: 19 [5120/16384 (31%)]\tLoss: 0.672518\n",
      "Train Epoch: 19 [10240/16384 (62%)]\tLoss: 0.705295\n",
      "Train Epoch: 19 [15360/16384 (94%)]\tLoss: 0.676858\n",
      "Test set: Average loss: 0.6879, Accuracy: 2077/4000 (52%)\n",
      "\n",
      "Train Epoch: 20 [0/16384 (0%)]\tLoss: 0.684603\n",
      "Train Epoch: 20 [5120/16384 (31%)]\tLoss: 0.679753\n",
      "Train Epoch: 20 [10240/16384 (62%)]\tLoss: 0.676884\n",
      "Train Epoch: 20 [15360/16384 (94%)]\tLoss: 0.677931\n",
      "Test set: Average loss: 0.6986, Accuracy: 2038/4000 (51%)\n",
      "\n",
      "Train Epoch: 21 [0/16384 (0%)]\tLoss: 0.708213\n",
      "Train Epoch: 21 [5120/16384 (31%)]\tLoss: 0.676028\n",
      "Train Epoch: 21 [10240/16384 (62%)]\tLoss: 0.683160\n",
      "Train Epoch: 21 [15360/16384 (94%)]\tLoss: 0.693272\n",
      "Test set: Average loss: 0.7100, Accuracy: 2047/4000 (51%)\n",
      "\n",
      "Train Epoch: 22 [0/16384 (0%)]\tLoss: 0.704769\n",
      "Train Epoch: 22 [5120/16384 (31%)]\tLoss: 0.671860\n",
      "Train Epoch: 22 [10240/16384 (62%)]\tLoss: 0.671357\n",
      "Train Epoch: 22 [15360/16384 (94%)]\tLoss: 0.667239\n",
      "Test set: Average loss: 0.6739, Accuracy: 2258/4000 (56%)\n",
      "\n",
      "Train Epoch: 23 [0/16384 (0%)]\tLoss: 0.678089\n",
      "Train Epoch: 23 [5120/16384 (31%)]\tLoss: 0.678903\n",
      "Train Epoch: 23 [10240/16384 (62%)]\tLoss: 0.679032\n",
      "Train Epoch: 23 [15360/16384 (94%)]\tLoss: 0.685900\n",
      "Test set: Average loss: 0.7053, Accuracy: 2034/4000 (51%)\n",
      "\n",
      "Train Epoch: 24 [0/16384 (0%)]\tLoss: 0.693510\n",
      "Train Epoch: 24 [5120/16384 (31%)]\tLoss: 0.661908\n",
      "Train Epoch: 24 [10240/16384 (62%)]\tLoss: 0.680855\n",
      "Train Epoch: 24 [15360/16384 (94%)]\tLoss: 0.714313\n",
      "Test set: Average loss: 0.6718, Accuracy: 2420/4000 (60%)\n",
      "\n",
      "Best acc:  0.605\n",
      "Train Epoch: 25 [0/16384 (0%)]\tLoss: 0.675388\n",
      "Train Epoch: 25 [5120/16384 (31%)]\tLoss: 0.676697\n",
      "Train Epoch: 25 [10240/16384 (62%)]\tLoss: 0.662891\n",
      "Train Epoch: 25 [15360/16384 (94%)]\tLoss: 0.678513\n",
      "Test set: Average loss: 0.6757, Accuracy: 2330/4000 (58%)\n",
      "\n",
      "Train Epoch: 26 [0/16384 (0%)]\tLoss: 0.685987\n",
      "Train Epoch: 26 [5120/16384 (31%)]\tLoss: 0.666281\n",
      "Train Epoch: 26 [10240/16384 (62%)]\tLoss: 0.665997\n",
      "Train Epoch: 26 [15360/16384 (94%)]\tLoss: 0.653755\n",
      "Test set: Average loss: 0.6579, Accuracy: 2468/4000 (62%)\n",
      "\n",
      "Best acc:  0.617\n",
      "Train Epoch: 27 [0/16384 (0%)]\tLoss: 0.659269\n",
      "Train Epoch: 27 [5120/16384 (31%)]\tLoss: 0.656447\n",
      "Train Epoch: 27 [10240/16384 (62%)]\tLoss: 0.709979\n",
      "Train Epoch: 27 [15360/16384 (94%)]\tLoss: 0.654997\n",
      "Test set: Average loss: 0.6515, Accuracy: 2502/4000 (63%)\n",
      "\n",
      "Best acc:  0.6255\n",
      "Train Epoch: 28 [0/16384 (0%)]\tLoss: 0.661887\n",
      "Train Epoch: 28 [5120/16384 (31%)]\tLoss: 0.643746\n",
      "Train Epoch: 28 [10240/16384 (62%)]\tLoss: 0.664600\n",
      "Train Epoch: 28 [15360/16384 (94%)]\tLoss: 0.659256\n",
      "Test set: Average loss: 0.7012, Accuracy: 2166/4000 (54%)\n",
      "\n",
      "Train Epoch: 29 [0/16384 (0%)]\tLoss: 0.711224\n",
      "Train Epoch: 29 [5120/16384 (31%)]\tLoss: 0.677720\n",
      "Train Epoch: 29 [10240/16384 (62%)]\tLoss: 0.663726\n",
      "Train Epoch: 29 [15360/16384 (94%)]\tLoss: 0.666067\n",
      "Test set: Average loss: 0.7441, Accuracy: 1972/4000 (49%)\n",
      "\n",
      "Train Epoch: 30 [0/16384 (0%)]\tLoss: 0.733136\n",
      "Train Epoch: 30 [5120/16384 (31%)]\tLoss: 0.678134\n",
      "Train Epoch: 30 [10240/16384 (62%)]\tLoss: 0.673149\n",
      "Train Epoch: 30 [15360/16384 (94%)]\tLoss: 0.705838\n",
      "Test set: Average loss: 0.6968, Accuracy: 2047/4000 (51%)\n",
      "\n",
      "Train Epoch: 31 [0/16384 (0%)]\tLoss: 0.686954\n",
      "Train Epoch: 31 [5120/16384 (31%)]\tLoss: 0.658050\n",
      "Train Epoch: 31 [10240/16384 (62%)]\tLoss: 0.658998\n",
      "Train Epoch: 31 [15360/16384 (94%)]\tLoss: 0.668256\n",
      "Test set: Average loss: 0.6473, Accuracy: 2545/4000 (64%)\n",
      "\n",
      "Best acc:  0.63625\n",
      "Train Epoch: 32 [0/16384 (0%)]\tLoss: 0.640194\n",
      "Train Epoch: 32 [5120/16384 (31%)]\tLoss: 0.659068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [10240/16384 (62%)]\tLoss: 0.655869\n",
      "Train Epoch: 32 [15360/16384 (94%)]\tLoss: 0.675687\n",
      "Test set: Average loss: 0.6487, Accuracy: 2525/4000 (63%)\n",
      "\n",
      "Train Epoch: 33 [0/16384 (0%)]\tLoss: 0.644999\n",
      "Train Epoch: 33 [5120/16384 (31%)]\tLoss: 0.638892\n",
      "Train Epoch: 33 [10240/16384 (62%)]\tLoss: 0.664310\n",
      "Train Epoch: 33 [15360/16384 (94%)]\tLoss: 0.660358\n",
      "Test set: Average loss: 0.6750, Accuracy: 2315/4000 (58%)\n",
      "\n",
      "Train Epoch: 34 [0/16384 (0%)]\tLoss: 0.657733\n",
      "Train Epoch: 34 [5120/16384 (31%)]\tLoss: 0.671289\n",
      "Train Epoch: 34 [10240/16384 (62%)]\tLoss: 0.642915\n",
      "Train Epoch: 34 [15360/16384 (94%)]\tLoss: 0.644728\n",
      "Test set: Average loss: 0.6602, Accuracy: 2412/4000 (60%)\n",
      "\n",
      "Train Epoch: 35 [0/16384 (0%)]\tLoss: 0.648360\n",
      "Train Epoch: 35 [5120/16384 (31%)]\tLoss: 0.639721\n",
      "Train Epoch: 35 [10240/16384 (62%)]\tLoss: 0.638448\n",
      "Train Epoch: 35 [15360/16384 (94%)]\tLoss: 0.631906\n",
      "Test set: Average loss: 0.6994, Accuracy: 2282/4000 (57%)\n",
      "\n",
      "Train Epoch: 36 [0/16384 (0%)]\tLoss: 0.675631\n",
      "Train Epoch: 36 [5120/16384 (31%)]\tLoss: 0.627289\n",
      "Train Epoch: 36 [10240/16384 (62%)]\tLoss: 0.641734\n",
      "Train Epoch: 36 [15360/16384 (94%)]\tLoss: 0.620811\n",
      "Test set: Average loss: 0.6232, Accuracy: 2670/4000 (67%)\n",
      "\n",
      "Best acc:  0.6675\n",
      "Train Epoch: 37 [0/16384 (0%)]\tLoss: 0.627388\n",
      "Train Epoch: 37 [5120/16384 (31%)]\tLoss: 0.666393\n",
      "Train Epoch: 37 [10240/16384 (62%)]\tLoss: 0.645891\n",
      "Train Epoch: 37 [15360/16384 (94%)]\tLoss: 0.681208\n",
      "Test set: Average loss: 0.6177, Accuracy: 2684/4000 (67%)\n",
      "\n",
      "Best acc:  0.671\n",
      "Train Epoch: 38 [0/16384 (0%)]\tLoss: 0.626469\n",
      "Train Epoch: 38 [5120/16384 (31%)]\tLoss: 0.653374\n",
      "Train Epoch: 38 [10240/16384 (62%)]\tLoss: 0.622603\n",
      "Train Epoch: 38 [15360/16384 (94%)]\tLoss: 0.629014\n",
      "Test set: Average loss: 0.6498, Accuracy: 2469/4000 (62%)\n",
      "\n",
      "Train Epoch: 39 [0/16384 (0%)]\tLoss: 0.637364\n",
      "Train Epoch: 39 [5120/16384 (31%)]\tLoss: 0.625499\n",
      "Train Epoch: 39 [10240/16384 (62%)]\tLoss: 0.626286\n",
      "Train Epoch: 39 [15360/16384 (94%)]\tLoss: 0.667636\n",
      "Test set: Average loss: 0.6571, Accuracy: 2417/4000 (60%)\n",
      "\n",
      "Train Epoch: 40 [0/16384 (0%)]\tLoss: 0.655048\n",
      "Train Epoch: 40 [5120/16384 (31%)]\tLoss: 0.621353\n",
      "Train Epoch: 40 [10240/16384 (62%)]\tLoss: 0.602309\n",
      "Train Epoch: 40 [15360/16384 (94%)]\tLoss: 0.622628\n",
      "Test set: Average loss: 0.6054, Accuracy: 2737/4000 (68%)\n",
      "\n",
      "Best acc:  0.68425\n",
      "Train Epoch: 41 [0/16384 (0%)]\tLoss: 0.602489\n",
      "Train Epoch: 41 [5120/16384 (31%)]\tLoss: 0.639802\n",
      "Train Epoch: 41 [10240/16384 (62%)]\tLoss: 0.627025\n",
      "Train Epoch: 41 [15360/16384 (94%)]\tLoss: 0.620267\n",
      "Test set: Average loss: 0.6127, Accuracy: 2702/4000 (68%)\n",
      "\n",
      "Train Epoch: 42 [0/16384 (0%)]\tLoss: 0.617645\n",
      "Train Epoch: 42 [5120/16384 (31%)]\tLoss: 0.682343\n",
      "Train Epoch: 42 [10240/16384 (62%)]\tLoss: 0.634453\n",
      "Train Epoch: 42 [15360/16384 (94%)]\tLoss: 0.629307\n",
      "Test set: Average loss: 0.6118, Accuracy: 2729/4000 (68%)\n",
      "\n",
      "Train Epoch: 43 [0/16384 (0%)]\tLoss: 0.603138\n",
      "Train Epoch: 43 [5120/16384 (31%)]\tLoss: 0.600868\n",
      "Train Epoch: 43 [10240/16384 (62%)]\tLoss: 0.651561\n",
      "Train Epoch: 43 [15360/16384 (94%)]\tLoss: 0.619696\n",
      "Test set: Average loss: 0.6283, Accuracy: 2604/4000 (65%)\n",
      "\n",
      "Train Epoch: 44 [0/16384 (0%)]\tLoss: 0.612850\n",
      "Train Epoch: 44 [5120/16384 (31%)]\tLoss: 0.654972\n",
      "Train Epoch: 44 [10240/16384 (62%)]\tLoss: 0.617990\n",
      "Train Epoch: 44 [15360/16384 (94%)]\tLoss: 0.606468\n",
      "Test set: Average loss: 0.6038, Accuracy: 2723/4000 (68%)\n",
      "\n",
      "Train Epoch: 45 [0/16384 (0%)]\tLoss: 0.594099\n",
      "Train Epoch: 45 [5120/16384 (31%)]\tLoss: 0.583239\n",
      "Train Epoch: 45 [10240/16384 (62%)]\tLoss: 0.623651\n",
      "Train Epoch: 45 [15360/16384 (94%)]\tLoss: 0.585119\n",
      "Test set: Average loss: 0.6114, Accuracy: 2697/4000 (67%)\n",
      "\n",
      "Train Epoch: 46 [0/16384 (0%)]\tLoss: 0.609259\n",
      "Train Epoch: 46 [5120/16384 (31%)]\tLoss: 0.617993\n",
      "Train Epoch: 46 [10240/16384 (62%)]\tLoss: 0.608535\n",
      "Train Epoch: 46 [15360/16384 (94%)]\tLoss: 0.688648\n",
      "Test set: Average loss: 0.6090, Accuracy: 2730/4000 (68%)\n",
      "\n",
      "Train Epoch: 47 [0/16384 (0%)]\tLoss: 0.614551\n",
      "Train Epoch: 47 [5120/16384 (31%)]\tLoss: 0.590091\n",
      "Train Epoch: 47 [10240/16384 (62%)]\tLoss: 0.581881\n",
      "Train Epoch: 47 [15360/16384 (94%)]\tLoss: 0.625650\n",
      "Test set: Average loss: 0.6115, Accuracy: 2710/4000 (68%)\n",
      "\n",
      "Train Epoch: 48 [0/16384 (0%)]\tLoss: 0.601316\n",
      "Train Epoch: 48 [5120/16384 (31%)]\tLoss: 0.594368\n",
      "Train Epoch: 48 [10240/16384 (62%)]\tLoss: 0.626467\n",
      "Train Epoch: 48 [15360/16384 (94%)]\tLoss: 0.592374\n",
      "Test set: Average loss: 0.6101, Accuracy: 2747/4000 (69%)\n",
      "\n",
      "Best acc:  0.68675\n",
      "Train Epoch: 49 [0/16384 (0%)]\tLoss: 0.637702\n",
      "Train Epoch: 49 [5120/16384 (31%)]\tLoss: 0.587421\n",
      "Train Epoch: 49 [10240/16384 (62%)]\tLoss: 0.572749\n",
      "Train Epoch: 49 [15360/16384 (94%)]\tLoss: 0.604150\n",
      "Test set: Average loss: 0.7392, Accuracy: 2168/4000 (54%)\n",
      "\n",
      "Train Epoch: 50 [0/16384 (0%)]\tLoss: 0.730962\n",
      "Train Epoch: 50 [5120/16384 (31%)]\tLoss: 0.583291\n",
      "Train Epoch: 50 [10240/16384 (62%)]\tLoss: 0.596235\n",
      "Train Epoch: 50 [15360/16384 (94%)]\tLoss: 0.594195\n",
      "Test set: Average loss: 0.6241, Accuracy: 2689/4000 (67%)\n",
      "\n",
      "Train Epoch: 51 [0/16384 (0%)]\tLoss: 0.652739\n",
      "Train Epoch: 51 [5120/16384 (31%)]\tLoss: 0.565365\n",
      "Train Epoch: 51 [10240/16384 (62%)]\tLoss: 0.569194\n",
      "Train Epoch: 51 [15360/16384 (94%)]\tLoss: 0.708614\n",
      "Test set: Average loss: 0.5852, Accuracy: 2829/4000 (71%)\n",
      "\n",
      "Best acc:  0.70725\n",
      "Train Epoch: 52 [0/16384 (0%)]\tLoss: 0.569779\n",
      "Train Epoch: 52 [5120/16384 (31%)]\tLoss: 0.612449\n",
      "Train Epoch: 52 [10240/16384 (62%)]\tLoss: 0.643054\n",
      "Train Epoch: 52 [15360/16384 (94%)]\tLoss: 0.563154\n",
      "Test set: Average loss: 0.5720, Accuracy: 2903/4000 (73%)\n",
      "\n",
      "Best acc:  0.72575\n",
      "Train Epoch: 53 [0/16384 (0%)]\tLoss: 0.553758\n",
      "Train Epoch: 53 [5120/16384 (31%)]\tLoss: 0.609231\n",
      "Train Epoch: 53 [10240/16384 (62%)]\tLoss: 0.631220\n",
      "Train Epoch: 53 [15360/16384 (94%)]\tLoss: 0.548442\n",
      "Test set: Average loss: 0.5900, Accuracy: 2810/4000 (70%)\n",
      "\n",
      "Train Epoch: 54 [0/16384 (0%)]\tLoss: 0.594303\n",
      "Train Epoch: 54 [5120/16384 (31%)]\tLoss: 0.581354\n",
      "Train Epoch: 54 [10240/16384 (62%)]\tLoss: 0.566763\n",
      "Train Epoch: 54 [15360/16384 (94%)]\tLoss: 0.583361\n",
      "Test set: Average loss: 0.5761, Accuracy: 2877/4000 (72%)\n",
      "\n",
      "Train Epoch: 55 [0/16384 (0%)]\tLoss: 0.576789\n",
      "Train Epoch: 55 [5120/16384 (31%)]\tLoss: 0.558473\n",
      "Train Epoch: 55 [10240/16384 (62%)]\tLoss: 0.615878\n",
      "Train Epoch: 55 [15360/16384 (94%)]\tLoss: 0.566374\n",
      "Test set: Average loss: 0.6083, Accuracy: 2714/4000 (68%)\n",
      "\n",
      "Train Epoch: 56 [0/16384 (0%)]\tLoss: 0.604874\n",
      "Train Epoch: 56 [5120/16384 (31%)]\tLoss: 0.573612\n",
      "Train Epoch: 56 [10240/16384 (62%)]\tLoss: 0.607980\n",
      "Train Epoch: 56 [15360/16384 (94%)]\tLoss: 0.600401\n",
      "Test set: Average loss: 0.5729, Accuracy: 2870/4000 (72%)\n",
      "\n",
      "Train Epoch: 57 [0/16384 (0%)]\tLoss: 0.556377\n",
      "Train Epoch: 57 [5120/16384 (31%)]\tLoss: 0.554908\n",
      "Train Epoch: 57 [10240/16384 (62%)]\tLoss: 0.585551\n",
      "Train Epoch: 57 [15360/16384 (94%)]\tLoss: 0.638065\n",
      "Test set: Average loss: 0.7112, Accuracy: 2131/4000 (53%)\n",
      "\n",
      "Train Epoch: 58 [0/16384 (0%)]\tLoss: 0.709096\n",
      "Train Epoch: 58 [5120/16384 (31%)]\tLoss: 0.562984\n",
      "Train Epoch: 58 [10240/16384 (62%)]\tLoss: 0.615886\n",
      "Train Epoch: 58 [15360/16384 (94%)]\tLoss: 0.543542\n",
      "Test set: Average loss: 0.5773, Accuracy: 2866/4000 (72%)\n",
      "\n",
      "Train Epoch: 59 [0/16384 (0%)]\tLoss: 0.562740\n",
      "Train Epoch: 59 [5120/16384 (31%)]\tLoss: 0.550249\n",
      "Train Epoch: 59 [10240/16384 (62%)]\tLoss: 0.551940\n",
      "Train Epoch: 59 [15360/16384 (94%)]\tLoss: 0.556647\n",
      "Test set: Average loss: 0.5679, Accuracy: 2920/4000 (73%)\n",
      "\n",
      "Best acc:  0.73\n",
      "Train Epoch: 60 [0/16384 (0%)]\tLoss: 0.551126\n",
      "Train Epoch: 60 [5120/16384 (31%)]\tLoss: 0.548425\n",
      "Train Epoch: 60 [10240/16384 (62%)]\tLoss: 0.561370\n",
      "Train Epoch: 60 [15360/16384 (94%)]\tLoss: 0.577930\n",
      "Test set: Average loss: 0.6396, Accuracy: 2583/4000 (65%)\n",
      "\n",
      "Train Epoch: 61 [0/16384 (0%)]\tLoss: 0.640454\n",
      "Train Epoch: 61 [5120/16384 (31%)]\tLoss: 0.597955\n",
      "Train Epoch: 61 [10240/16384 (62%)]\tLoss: 0.562298\n",
      "Train Epoch: 61 [15360/16384 (94%)]\tLoss: 0.558935\n",
      "Test set: Average loss: 0.5784, Accuracy: 2849/4000 (71%)\n",
      "\n",
      "Train Epoch: 62 [0/16384 (0%)]\tLoss: 0.610590\n",
      "Train Epoch: 62 [5120/16384 (31%)]\tLoss: 0.565683\n",
      "Train Epoch: 62 [10240/16384 (62%)]\tLoss: 0.564363\n",
      "Train Epoch: 62 [15360/16384 (94%)]\tLoss: 0.540921\n",
      "Test set: Average loss: 0.6172, Accuracy: 2717/4000 (68%)\n",
      "\n",
      "Train Epoch: 63 [0/16384 (0%)]\tLoss: 0.605688\n",
      "Train Epoch: 63 [5120/16384 (31%)]\tLoss: 0.543782\n",
      "Train Epoch: 63 [10240/16384 (62%)]\tLoss: 0.529886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 [15360/16384 (94%)]\tLoss: 0.534071\n",
      "Test set: Average loss: 0.5485, Accuracy: 3002/4000 (75%)\n",
      "\n",
      "Best acc:  0.7505\n",
      "Train Epoch: 64 [0/16384 (0%)]\tLoss: 0.516847\n",
      "Train Epoch: 64 [5120/16384 (31%)]\tLoss: 0.542553\n",
      "Train Epoch: 64 [10240/16384 (62%)]\tLoss: 0.540210\n",
      "Train Epoch: 64 [15360/16384 (94%)]\tLoss: 0.527962\n",
      "Test set: Average loss: 0.5791, Accuracy: 2880/4000 (72%)\n",
      "\n",
      "Train Epoch: 65 [0/16384 (0%)]\tLoss: 0.564232\n",
      "Train Epoch: 65 [5120/16384 (31%)]\tLoss: 0.536872\n",
      "Train Epoch: 65 [10240/16384 (62%)]\tLoss: 0.523727\n",
      "Train Epoch: 65 [15360/16384 (94%)]\tLoss: 0.549923\n",
      "Test set: Average loss: 0.5701, Accuracy: 2884/4000 (72%)\n",
      "\n",
      "Train Epoch: 66 [0/16384 (0%)]\tLoss: 0.570424\n",
      "Train Epoch: 66 [5120/16384 (31%)]\tLoss: 0.559142\n",
      "Train Epoch: 66 [10240/16384 (62%)]\tLoss: 0.508360\n",
      "Train Epoch: 66 [15360/16384 (94%)]\tLoss: 0.552999\n",
      "Test set: Average loss: 0.5911, Accuracy: 2807/4000 (70%)\n",
      "\n",
      "Train Epoch: 67 [0/16384 (0%)]\tLoss: 0.553862\n",
      "Train Epoch: 67 [5120/16384 (31%)]\tLoss: 0.577871\n",
      "Train Epoch: 67 [10240/16384 (62%)]\tLoss: 0.517025\n",
      "Train Epoch: 67 [15360/16384 (94%)]\tLoss: 0.528991\n",
      "Test set: Average loss: 0.5531, Accuracy: 2987/4000 (75%)\n",
      "\n",
      "Train Epoch: 68 [0/16384 (0%)]\tLoss: 0.542321\n",
      "Train Epoch: 68 [5120/16384 (31%)]\tLoss: 0.596106\n",
      "Train Epoch: 68 [10240/16384 (62%)]\tLoss: 0.564358\n",
      "Train Epoch: 68 [15360/16384 (94%)]\tLoss: 0.539034\n",
      "Test set: Average loss: 0.6427, Accuracy: 2564/4000 (64%)\n",
      "\n",
      "Train Epoch: 69 [0/16384 (0%)]\tLoss: 0.638887\n",
      "Train Epoch: 69 [5120/16384 (31%)]\tLoss: 0.514944\n",
      "Train Epoch: 69 [10240/16384 (62%)]\tLoss: 0.593072\n",
      "Train Epoch: 69 [15360/16384 (94%)]\tLoss: 0.561679\n",
      "Test set: Average loss: 0.6103, Accuracy: 2705/4000 (68%)\n",
      "\n",
      "Train Epoch: 70 [0/16384 (0%)]\tLoss: 0.597168\n",
      "Train Epoch: 70 [5120/16384 (31%)]\tLoss: 0.537619\n",
      "Train Epoch: 70 [10240/16384 (62%)]\tLoss: 0.589951\n",
      "Train Epoch: 70 [15360/16384 (94%)]\tLoss: 0.564815\n",
      "Test set: Average loss: 0.5601, Accuracy: 2940/4000 (74%)\n",
      "\n",
      "Train Epoch: 71 [0/16384 (0%)]\tLoss: 0.548226\n",
      "Train Epoch: 71 [5120/16384 (31%)]\tLoss: 0.542079\n",
      "Train Epoch: 71 [10240/16384 (62%)]\tLoss: 0.545384\n",
      "Train Epoch: 71 [15360/16384 (94%)]\tLoss: 0.516904\n",
      "Test set: Average loss: 0.5574, Accuracy: 2960/4000 (74%)\n",
      "\n",
      "Train Epoch: 72 [0/16384 (0%)]\tLoss: 0.501940\n",
      "Train Epoch: 72 [5120/16384 (31%)]\tLoss: 0.556064\n",
      "Train Epoch: 72 [10240/16384 (62%)]\tLoss: 0.545268\n",
      "Train Epoch: 72 [15360/16384 (94%)]\tLoss: 0.532282\n",
      "Test set: Average loss: 0.5350, Accuracy: 3043/4000 (76%)\n",
      "\n",
      "Best acc:  0.76075\n",
      "Train Epoch: 73 [0/16384 (0%)]\tLoss: 0.515940\n",
      "Train Epoch: 73 [5120/16384 (31%)]\tLoss: 0.524692\n",
      "Train Epoch: 73 [10240/16384 (62%)]\tLoss: 0.514912\n",
      "Train Epoch: 73 [15360/16384 (94%)]\tLoss: 0.551216\n",
      "Test set: Average loss: 0.5670, Accuracy: 2909/4000 (73%)\n",
      "\n",
      "Train Epoch: 74 [0/16384 (0%)]\tLoss: 0.530672\n",
      "Train Epoch: 74 [5120/16384 (31%)]\tLoss: 0.543704\n",
      "Train Epoch: 74 [10240/16384 (62%)]\tLoss: 0.556527\n",
      "Train Epoch: 74 [15360/16384 (94%)]\tLoss: 0.572473\n",
      "Test set: Average loss: 0.5878, Accuracy: 2813/4000 (70%)\n",
      "\n",
      "Train Epoch: 75 [0/16384 (0%)]\tLoss: 0.574717\n",
      "Train Epoch: 75 [5120/16384 (31%)]\tLoss: 0.517008\n",
      "Train Epoch: 75 [10240/16384 (62%)]\tLoss: 0.550018\n",
      "Train Epoch: 75 [15360/16384 (94%)]\tLoss: 0.536192\n",
      "Test set: Average loss: 0.5345, Accuracy: 3059/4000 (76%)\n",
      "\n",
      "Best acc:  0.76475\n",
      "Train Epoch: 76 [0/16384 (0%)]\tLoss: 0.516976\n",
      "Train Epoch: 76 [5120/16384 (31%)]\tLoss: 0.530334\n",
      "Train Epoch: 76 [10240/16384 (62%)]\tLoss: 0.496781\n",
      "Train Epoch: 76 [15360/16384 (94%)]\tLoss: 0.525222\n",
      "Test set: Average loss: 0.5444, Accuracy: 2994/4000 (75%)\n",
      "\n",
      "Train Epoch: 77 [0/16384 (0%)]\tLoss: 0.535187\n",
      "Train Epoch: 77 [5120/16384 (31%)]\tLoss: 0.644394\n",
      "Train Epoch: 77 [10240/16384 (62%)]\tLoss: 0.548141\n",
      "Train Epoch: 77 [15360/16384 (94%)]\tLoss: 0.544203\n",
      "Test set: Average loss: 0.5387, Accuracy: 3045/4000 (76%)\n",
      "\n",
      "Train Epoch: 78 [0/16384 (0%)]\tLoss: 0.532339\n",
      "Train Epoch: 78 [5120/16384 (31%)]\tLoss: 0.706599\n",
      "Train Epoch: 78 [10240/16384 (62%)]\tLoss: 0.489842\n",
      "Train Epoch: 78 [15360/16384 (94%)]\tLoss: 0.482084\n",
      "Test set: Average loss: 0.5560, Accuracy: 2972/4000 (74%)\n",
      "\n",
      "Train Epoch: 79 [0/16384 (0%)]\tLoss: 0.527067\n",
      "Train Epoch: 79 [5120/16384 (31%)]\tLoss: 0.501120\n",
      "Train Epoch: 79 [10240/16384 (62%)]\tLoss: 0.570365\n",
      "Train Epoch: 79 [15360/16384 (94%)]\tLoss: 0.508808\n",
      "Test set: Average loss: 0.5300, Accuracy: 3094/4000 (77%)\n",
      "\n",
      "Best acc:  0.7735\n",
      "Train Epoch: 80 [0/16384 (0%)]\tLoss: 0.527916\n",
      "Train Epoch: 80 [5120/16384 (31%)]\tLoss: 0.553295\n",
      "Train Epoch: 80 [10240/16384 (62%)]\tLoss: 0.501722\n",
      "Train Epoch: 80 [15360/16384 (94%)]\tLoss: 0.493821\n",
      "Test set: Average loss: 0.5534, Accuracy: 2965/4000 (74%)\n",
      "\n",
      "Train Epoch: 81 [0/16384 (0%)]\tLoss: 0.523476\n",
      "Train Epoch: 81 [5120/16384 (31%)]\tLoss: 0.510303\n",
      "Train Epoch: 81 [10240/16384 (62%)]\tLoss: 0.498989\n",
      "Train Epoch: 81 [15360/16384 (94%)]\tLoss: 0.494899\n",
      "Test set: Average loss: 0.5435, Accuracy: 3043/4000 (76%)\n",
      "\n",
      "Train Epoch: 82 [0/16384 (0%)]\tLoss: 0.509296\n",
      "Train Epoch: 82 [5120/16384 (31%)]\tLoss: 0.508727\n",
      "Train Epoch: 82 [10240/16384 (62%)]\tLoss: 0.553523\n",
      "Train Epoch: 82 [15360/16384 (94%)]\tLoss: 0.487156\n",
      "Test set: Average loss: 0.5278, Accuracy: 3089/4000 (77%)\n",
      "\n",
      "Train Epoch: 83 [0/16384 (0%)]\tLoss: 0.495718\n",
      "Train Epoch: 83 [5120/16384 (31%)]\tLoss: 0.493954\n",
      "Train Epoch: 83 [10240/16384 (62%)]\tLoss: 0.497499\n",
      "Train Epoch: 83 [15360/16384 (94%)]\tLoss: 0.503888\n",
      "Test set: Average loss: 0.5708, Accuracy: 2869/4000 (72%)\n",
      "\n",
      "Train Epoch: 84 [0/16384 (0%)]\tLoss: 0.571335\n",
      "Train Epoch: 84 [5120/16384 (31%)]\tLoss: 0.482478\n",
      "Train Epoch: 84 [10240/16384 (62%)]\tLoss: 0.491327\n",
      "Train Epoch: 84 [15360/16384 (94%)]\tLoss: 0.483547\n",
      "Test set: Average loss: 0.5492, Accuracy: 3000/4000 (75%)\n",
      "\n",
      "Train Epoch: 85 [0/16384 (0%)]\tLoss: 0.581329\n",
      "Train Epoch: 85 [5120/16384 (31%)]\tLoss: 0.535360\n",
      "Train Epoch: 85 [10240/16384 (62%)]\tLoss: 0.532842\n",
      "Train Epoch: 85 [15360/16384 (94%)]\tLoss: 0.472435\n",
      "Test set: Average loss: 0.5795, Accuracy: 2886/4000 (72%)\n",
      "\n",
      "Train Epoch: 86 [0/16384 (0%)]\tLoss: 0.567017\n",
      "Train Epoch: 86 [5120/16384 (31%)]\tLoss: 0.505449\n",
      "Train Epoch: 86 [10240/16384 (62%)]\tLoss: 0.499294\n",
      "Train Epoch: 86 [15360/16384 (94%)]\tLoss: 0.496521\n",
      "Test set: Average loss: 0.5011, Accuracy: 3207/4000 (80%)\n",
      "\n",
      "Best acc:  0.80175\n",
      "Train Epoch: 87 [0/16384 (0%)]\tLoss: 0.460806\n",
      "Train Epoch: 87 [5120/16384 (31%)]\tLoss: 0.516229\n",
      "Train Epoch: 87 [10240/16384 (62%)]\tLoss: 0.536902\n",
      "Train Epoch: 87 [15360/16384 (94%)]\tLoss: 0.506766\n",
      "Test set: Average loss: 0.5217, Accuracy: 3126/4000 (78%)\n",
      "\n",
      "Train Epoch: 88 [0/16384 (0%)]\tLoss: 0.503668\n",
      "Train Epoch: 88 [5120/16384 (31%)]\tLoss: 0.483880\n",
      "Train Epoch: 88 [10240/16384 (62%)]\tLoss: 0.474956\n",
      "Train Epoch: 88 [15360/16384 (94%)]\tLoss: 0.484589\n",
      "Test set: Average loss: 0.5223, Accuracy: 3099/4000 (77%)\n",
      "\n",
      "Train Epoch: 89 [0/16384 (0%)]\tLoss: 0.483938\n",
      "Train Epoch: 89 [5120/16384 (31%)]\tLoss: 0.515483\n",
      "Train Epoch: 89 [10240/16384 (62%)]\tLoss: 0.499381\n",
      "Train Epoch: 89 [15360/16384 (94%)]\tLoss: 0.510667\n",
      "Test set: Average loss: 0.6836, Accuracy: 2465/4000 (62%)\n",
      "\n",
      "Train Epoch: 90 [0/16384 (0%)]\tLoss: 0.700661\n",
      "Train Epoch: 90 [5120/16384 (31%)]\tLoss: 0.508606\n",
      "Train Epoch: 90 [10240/16384 (62%)]\tLoss: 0.469845\n",
      "Train Epoch: 90 [15360/16384 (94%)]\tLoss: 0.449696\n",
      "Test set: Average loss: 0.5758, Accuracy: 2881/4000 (72%)\n",
      "\n",
      "Train Epoch: 91 [0/16384 (0%)]\tLoss: 0.554715\n",
      "Train Epoch: 91 [5120/16384 (31%)]\tLoss: 0.503495\n",
      "Train Epoch: 91 [10240/16384 (62%)]\tLoss: 0.502808\n",
      "Train Epoch: 91 [15360/16384 (94%)]\tLoss: 0.616453\n",
      "Test set: Average loss: 0.5748, Accuracy: 2864/4000 (72%)\n",
      "\n",
      "Train Epoch: 92 [0/16384 (0%)]\tLoss: 0.564258\n",
      "Train Epoch: 92 [5120/16384 (31%)]\tLoss: 0.481802\n",
      "Train Epoch: 92 [10240/16384 (62%)]\tLoss: 0.467484\n",
      "Train Epoch: 92 [15360/16384 (94%)]\tLoss: 0.484413\n",
      "Test set: Average loss: 0.6398, Accuracy: 2645/4000 (66%)\n",
      "\n",
      "Train Epoch: 93 [0/16384 (0%)]\tLoss: 0.654889\n",
      "Train Epoch: 93 [5120/16384 (31%)]\tLoss: 0.479730\n",
      "Train Epoch: 93 [10240/16384 (62%)]\tLoss: 0.470622\n",
      "Train Epoch: 93 [15360/16384 (94%)]\tLoss: 0.474301\n",
      "Test set: Average loss: 0.5102, Accuracy: 3163/4000 (79%)\n",
      "\n",
      "Train Epoch: 94 [0/16384 (0%)]\tLoss: 0.459687\n",
      "Train Epoch: 94 [5120/16384 (31%)]\tLoss: 0.517219\n",
      "Train Epoch: 94 [10240/16384 (62%)]\tLoss: 0.507562\n",
      "Train Epoch: 94 [15360/16384 (94%)]\tLoss: 0.483828\n",
      "Test set: Average loss: 0.5298, Accuracy: 3065/4000 (77%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [0/16384 (0%)]\tLoss: 0.481441\n",
      "Train Epoch: 95 [5120/16384 (31%)]\tLoss: 0.470030\n",
      "Train Epoch: 95 [10240/16384 (62%)]\tLoss: 0.533224\n",
      "Train Epoch: 95 [15360/16384 (94%)]\tLoss: 0.456252\n",
      "Test set: Average loss: 0.5217, Accuracy: 3115/4000 (78%)\n",
      "\n",
      "Train Epoch: 96 [0/16384 (0%)]\tLoss: 0.478412\n",
      "Train Epoch: 96 [5120/16384 (31%)]\tLoss: 0.467980\n",
      "Train Epoch: 96 [10240/16384 (62%)]\tLoss: 0.525587\n",
      "Train Epoch: 96 [15360/16384 (94%)]\tLoss: 0.489669\n",
      "Test set: Average loss: 0.5472, Accuracy: 3003/4000 (75%)\n",
      "\n",
      "Train Epoch: 97 [0/16384 (0%)]\tLoss: 0.521211\n",
      "Train Epoch: 97 [5120/16384 (31%)]\tLoss: 0.519761\n",
      "Train Epoch: 97 [10240/16384 (62%)]\tLoss: 0.454360\n",
      "Train Epoch: 97 [15360/16384 (94%)]\tLoss: 0.506478\n",
      "Test set: Average loss: 0.5633, Accuracy: 2951/4000 (74%)\n",
      "\n",
      "Train Epoch: 98 [0/16384 (0%)]\tLoss: 0.535312\n",
      "Train Epoch: 98 [5120/16384 (31%)]\tLoss: 0.514041\n",
      "Train Epoch: 98 [10240/16384 (62%)]\tLoss: 0.460860\n",
      "Train Epoch: 98 [15360/16384 (94%)]\tLoss: 0.441195\n",
      "Test set: Average loss: 0.5590, Accuracy: 2974/4000 (74%)\n",
      "\n",
      "Train Epoch: 99 [0/16384 (0%)]\tLoss: 0.527081\n",
      "Train Epoch: 99 [5120/16384 (31%)]\tLoss: 0.433132\n",
      "Train Epoch: 99 [10240/16384 (62%)]\tLoss: 0.494770\n",
      "Train Epoch: 99 [15360/16384 (94%)]\tLoss: 0.450734\n",
      "Test set: Average loss: 0.5016, Accuracy: 3212/4000 (80%)\n",
      "\n",
      "Best acc:  0.803\n",
      "Train Epoch: 100 [0/16384 (0%)]\tLoss: 0.443252\n",
      "Train Epoch: 100 [5120/16384 (31%)]\tLoss: 0.510129\n",
      "Train Epoch: 100 [10240/16384 (62%)]\tLoss: 0.451858\n",
      "Train Epoch: 100 [15360/16384 (94%)]\tLoss: 0.484780\n",
      "Test set: Average loss: 0.5177, Accuracy: 3142/4000 (79%)\n",
      "\n",
      "Train Epoch: 101 [0/16384 (0%)]\tLoss: 0.483858\n",
      "Train Epoch: 101 [5120/16384 (31%)]\tLoss: 0.484862\n",
      "Train Epoch: 101 [10240/16384 (62%)]\tLoss: 0.463973\n",
      "Train Epoch: 101 [15360/16384 (94%)]\tLoss: 0.434400\n",
      "Test set: Average loss: 0.5043, Accuracy: 3199/4000 (80%)\n",
      "\n",
      "Train Epoch: 102 [0/16384 (0%)]\tLoss: 0.462538\n",
      "Train Epoch: 102 [5120/16384 (31%)]\tLoss: 0.448829\n",
      "Train Epoch: 102 [10240/16384 (62%)]\tLoss: 0.506900\n",
      "Train Epoch: 102 [15360/16384 (94%)]\tLoss: 0.468670\n",
      "Test set: Average loss: 0.5258, Accuracy: 3102/4000 (78%)\n",
      "\n",
      "Train Epoch: 103 [0/16384 (0%)]\tLoss: 0.469191\n",
      "Train Epoch: 103 [5120/16384 (31%)]\tLoss: 0.439496\n",
      "Train Epoch: 103 [10240/16384 (62%)]\tLoss: 0.471934\n",
      "Train Epoch: 103 [15360/16384 (94%)]\tLoss: 0.478203\n",
      "Test set: Average loss: 0.6036, Accuracy: 2784/4000 (70%)\n",
      "\n",
      "Train Epoch: 104 [0/16384 (0%)]\tLoss: 0.621443\n",
      "Train Epoch: 104 [5120/16384 (31%)]\tLoss: 0.443431\n",
      "Train Epoch: 104 [10240/16384 (62%)]\tLoss: 0.473575\n",
      "Train Epoch: 104 [15360/16384 (94%)]\tLoss: 0.467070\n",
      "Test set: Average loss: 0.5117, Accuracy: 3168/4000 (79%)\n",
      "\n",
      "Train Epoch: 105 [0/16384 (0%)]\tLoss: 0.464543\n",
      "Train Epoch: 105 [5120/16384 (31%)]\tLoss: 0.501637\n",
      "Train Epoch: 105 [10240/16384 (62%)]\tLoss: 0.511670\n",
      "Train Epoch: 105 [15360/16384 (94%)]\tLoss: 0.477246\n",
      "Test set: Average loss: 0.4929, Accuracy: 3249/4000 (81%)\n",
      "\n",
      "Best acc:  0.81225\n",
      "Train Epoch: 106 [0/16384 (0%)]\tLoss: 0.458341\n",
      "Train Epoch: 106 [5120/16384 (31%)]\tLoss: 0.444745\n",
      "Train Epoch: 106 [10240/16384 (62%)]\tLoss: 0.597891\n",
      "Train Epoch: 106 [15360/16384 (94%)]\tLoss: 0.445565\n",
      "Test set: Average loss: 0.5098, Accuracy: 3160/4000 (79%)\n",
      "\n",
      "Train Epoch: 107 [0/16384 (0%)]\tLoss: 0.456727\n",
      "Train Epoch: 107 [5120/16384 (31%)]\tLoss: 0.493123\n",
      "Train Epoch: 107 [10240/16384 (62%)]\tLoss: 0.465633\n",
      "Train Epoch: 107 [15360/16384 (94%)]\tLoss: 0.445921\n",
      "Test set: Average loss: 0.5498, Accuracy: 2997/4000 (75%)\n",
      "\n",
      "Train Epoch: 108 [0/16384 (0%)]\tLoss: 0.502254\n",
      "Train Epoch: 108 [5120/16384 (31%)]\tLoss: 0.525619\n",
      "Train Epoch: 108 [10240/16384 (62%)]\tLoss: 0.476565\n",
      "Train Epoch: 108 [15360/16384 (94%)]\tLoss: 0.447683\n",
      "Test set: Average loss: 0.5469, Accuracy: 3010/4000 (75%)\n",
      "\n",
      "Train Epoch: 109 [0/16384 (0%)]\tLoss: 0.489545\n",
      "Train Epoch: 109 [5120/16384 (31%)]\tLoss: 0.418469\n",
      "Train Epoch: 109 [10240/16384 (62%)]\tLoss: 0.460257\n",
      "Train Epoch: 109 [15360/16384 (94%)]\tLoss: 0.426809\n",
      "Test set: Average loss: 0.5696, Accuracy: 2929/4000 (73%)\n",
      "\n",
      "Train Epoch: 110 [0/16384 (0%)]\tLoss: 0.539383\n",
      "Train Epoch: 110 [5120/16384 (31%)]\tLoss: 0.489190\n",
      "Train Epoch: 110 [10240/16384 (62%)]\tLoss: 0.430427\n",
      "Train Epoch: 110 [15360/16384 (94%)]\tLoss: 0.520904\n",
      "Test set: Average loss: 0.5163, Accuracy: 3151/4000 (79%)\n",
      "\n",
      "Train Epoch: 111 [0/16384 (0%)]\tLoss: 0.464477\n",
      "Train Epoch: 111 [5120/16384 (31%)]\tLoss: 0.449415\n",
      "Train Epoch: 111 [10240/16384 (62%)]\tLoss: 0.415384\n",
      "Train Epoch: 111 [15360/16384 (94%)]\tLoss: 0.476739\n",
      "Test set: Average loss: 0.5345, Accuracy: 3057/4000 (76%)\n",
      "\n",
      "Train Epoch: 112 [0/16384 (0%)]\tLoss: 0.481741\n",
      "Train Epoch: 112 [5120/16384 (31%)]\tLoss: 0.449064\n",
      "Train Epoch: 112 [10240/16384 (62%)]\tLoss: 0.429880\n",
      "Train Epoch: 112 [15360/16384 (94%)]\tLoss: 0.441460\n",
      "Test set: Average loss: 0.5383, Accuracy: 3064/4000 (77%)\n",
      "\n",
      "Train Epoch: 113 [0/16384 (0%)]\tLoss: 0.496352\n",
      "Train Epoch: 113 [5120/16384 (31%)]\tLoss: 0.502214\n",
      "Train Epoch: 113 [10240/16384 (62%)]\tLoss: 0.423502\n",
      "Train Epoch: 113 [15360/16384 (94%)]\tLoss: 0.421296\n",
      "Test set: Average loss: 0.5012, Accuracy: 3213/4000 (80%)\n",
      "\n",
      "Train Epoch: 114 [0/16384 (0%)]\tLoss: 0.428638\n",
      "Train Epoch: 114 [5120/16384 (31%)]\tLoss: 0.459025\n",
      "Train Epoch: 114 [10240/16384 (62%)]\tLoss: 0.437551\n",
      "Train Epoch: 114 [15360/16384 (94%)]\tLoss: 0.433434\n",
      "Test set: Average loss: 0.5421, Accuracy: 3049/4000 (76%)\n",
      "\n",
      "Train Epoch: 115 [0/16384 (0%)]\tLoss: 0.508788\n",
      "Train Epoch: 115 [5120/16384 (31%)]\tLoss: 0.443166\n",
      "Train Epoch: 115 [10240/16384 (62%)]\tLoss: 0.438290\n",
      "Train Epoch: 115 [15360/16384 (94%)]\tLoss: 0.454180\n",
      "Test set: Average loss: 0.5055, Accuracy: 3193/4000 (80%)\n",
      "\n",
      "Train Epoch: 116 [0/16384 (0%)]\tLoss: 0.440890\n",
      "Train Epoch: 116 [5120/16384 (31%)]\tLoss: 0.425295\n",
      "Train Epoch: 116 [10240/16384 (62%)]\tLoss: 0.395569\n",
      "Train Epoch: 116 [15360/16384 (94%)]\tLoss: 0.432052\n",
      "Test set: Average loss: 0.6489, Accuracy: 2604/4000 (65%)\n",
      "\n",
      "Train Epoch: 117 [0/16384 (0%)]\tLoss: 0.678619\n",
      "Train Epoch: 117 [5120/16384 (31%)]\tLoss: 0.467000\n",
      "Train Epoch: 117 [10240/16384 (62%)]\tLoss: 0.415117\n",
      "Train Epoch: 117 [15360/16384 (94%)]\tLoss: 0.444344\n",
      "Test set: Average loss: 0.5343, Accuracy: 3061/4000 (77%)\n",
      "\n",
      "Train Epoch: 118 [0/16384 (0%)]\tLoss: 0.477238\n",
      "Train Epoch: 118 [5120/16384 (31%)]\tLoss: 0.449522\n",
      "Train Epoch: 118 [10240/16384 (62%)]\tLoss: 0.441489\n",
      "Train Epoch: 118 [15360/16384 (94%)]\tLoss: 0.475689\n",
      "Test set: Average loss: 0.5151, Accuracy: 3164/4000 (79%)\n",
      "\n",
      "Train Epoch: 119 [0/16384 (0%)]\tLoss: 0.410333\n",
      "Train Epoch: 119 [5120/16384 (31%)]\tLoss: 0.437303\n",
      "Train Epoch: 119 [10240/16384 (62%)]\tLoss: 0.406007\n",
      "Train Epoch: 119 [15360/16384 (94%)]\tLoss: 0.420340\n",
      "Test set: Average loss: 0.5520, Accuracy: 2983/4000 (75%)\n",
      "\n",
      "Train Epoch: 120 [0/16384 (0%)]\tLoss: 0.496712\n",
      "Train Epoch: 120 [5120/16384 (31%)]\tLoss: 0.449717\n",
      "Train Epoch: 120 [10240/16384 (62%)]\tLoss: 0.418199\n",
      "Train Epoch: 120 [15360/16384 (94%)]\tLoss: 0.424957\n",
      "Test set: Average loss: 0.5948, Accuracy: 2847/4000 (71%)\n",
      "\n",
      "Train Epoch: 121 [0/16384 (0%)]\tLoss: 0.581155\n",
      "Train Epoch: 121 [5120/16384 (31%)]\tLoss: 0.469997\n",
      "Train Epoch: 121 [10240/16384 (62%)]\tLoss: 0.426312\n",
      "Train Epoch: 121 [15360/16384 (94%)]\tLoss: 0.437440\n",
      "Test set: Average loss: 0.5220, Accuracy: 3117/4000 (78%)\n",
      "\n",
      "Train Epoch: 122 [0/16384 (0%)]\tLoss: 0.461805\n",
      "Train Epoch: 122 [5120/16384 (31%)]\tLoss: 0.401644\n",
      "Train Epoch: 122 [10240/16384 (62%)]\tLoss: 0.556615\n",
      "Train Epoch: 122 [15360/16384 (94%)]\tLoss: 0.422208\n",
      "Test set: Average loss: 0.5835, Accuracy: 2888/4000 (72%)\n",
      "\n",
      "Train Epoch: 123 [0/16384 (0%)]\tLoss: 0.521943\n",
      "Train Epoch: 123 [5120/16384 (31%)]\tLoss: 0.412447\n",
      "Train Epoch: 123 [10240/16384 (62%)]\tLoss: 0.415032\n",
      "Train Epoch: 123 [15360/16384 (94%)]\tLoss: 0.431622\n",
      "Test set: Average loss: 0.5281, Accuracy: 3107/4000 (78%)\n",
      "\n",
      "Train Epoch: 124 [0/16384 (0%)]\tLoss: 0.465624\n",
      "Train Epoch: 124 [5120/16384 (31%)]\tLoss: 0.430364\n",
      "Train Epoch: 124 [10240/16384 (62%)]\tLoss: 0.402078\n",
      "Train Epoch: 124 [15360/16384 (94%)]\tLoss: 0.421798\n",
      "Test set: Average loss: 0.4964, Accuracy: 3241/4000 (81%)\n",
      "\n",
      "Train Epoch: 125 [0/16384 (0%)]\tLoss: 0.419797\n",
      "Train Epoch: 125 [5120/16384 (31%)]\tLoss: 0.406518\n",
      "Train Epoch: 125 [10240/16384 (62%)]\tLoss: 0.428820\n",
      "Train Epoch: 125 [15360/16384 (94%)]\tLoss: 0.466024\n",
      "Test set: Average loss: 0.5342, Accuracy: 3067/4000 (77%)\n",
      "\n",
      "Train Epoch: 126 [0/16384 (0%)]\tLoss: 0.484972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 126 [5120/16384 (31%)]\tLoss: 0.411619\n",
      "Train Epoch: 126 [10240/16384 (62%)]\tLoss: 0.443749\n",
      "Train Epoch: 126 [15360/16384 (94%)]\tLoss: 0.404100\n",
      "Test set: Average loss: 0.5361, Accuracy: 3079/4000 (77%)\n",
      "\n",
      "Train Epoch: 127 [0/16384 (0%)]\tLoss: 0.469146\n",
      "Train Epoch: 127 [5120/16384 (31%)]\tLoss: 0.417713\n",
      "Train Epoch: 127 [10240/16384 (62%)]\tLoss: 0.408782\n",
      "Train Epoch: 127 [15360/16384 (94%)]\tLoss: 0.425369\n",
      "Test set: Average loss: 0.4870, Accuracy: 3274/4000 (82%)\n",
      "\n",
      "Best acc:  0.8185\n",
      "Train Epoch: 128 [0/16384 (0%)]\tLoss: 0.427076\n",
      "Train Epoch: 128 [5120/16384 (31%)]\tLoss: 0.505193\n",
      "Train Epoch: 128 [10240/16384 (62%)]\tLoss: 0.387068\n",
      "Train Epoch: 128 [15360/16384 (94%)]\tLoss: 0.429159\n",
      "Test set: Average loss: 0.4942, Accuracy: 3243/4000 (81%)\n",
      "\n",
      "Train Epoch: 129 [0/16384 (0%)]\tLoss: 0.400035\n",
      "Train Epoch: 129 [5120/16384 (31%)]\tLoss: 0.505950\n",
      "Train Epoch: 129 [10240/16384 (62%)]\tLoss: 0.398476\n",
      "Train Epoch: 129 [15360/16384 (94%)]\tLoss: 0.436346\n",
      "Test set: Average loss: 0.4975, Accuracy: 3215/4000 (80%)\n",
      "\n",
      "Train Epoch: 130 [0/16384 (0%)]\tLoss: 0.405901\n",
      "Train Epoch: 130 [5120/16384 (31%)]\tLoss: 0.412379\n",
      "Train Epoch: 130 [10240/16384 (62%)]\tLoss: 0.412298\n",
      "Train Epoch: 130 [15360/16384 (94%)]\tLoss: 0.411845\n",
      "Test set: Average loss: 0.5648, Accuracy: 2947/4000 (74%)\n",
      "\n",
      "Train Epoch: 131 [0/16384 (0%)]\tLoss: 0.495926\n",
      "Train Epoch: 131 [5120/16384 (31%)]\tLoss: 0.398003\n",
      "Train Epoch: 131 [10240/16384 (62%)]\tLoss: 0.455552\n",
      "Train Epoch: 131 [15360/16384 (94%)]\tLoss: 0.386721\n",
      "Test set: Average loss: 0.5244, Accuracy: 3129/4000 (78%)\n",
      "\n",
      "Train Epoch: 132 [0/16384 (0%)]\tLoss: 0.460190\n",
      "Train Epoch: 132 [5120/16384 (31%)]\tLoss: 0.408300\n",
      "Train Epoch: 132 [10240/16384 (62%)]\tLoss: 0.400415\n",
      "Train Epoch: 132 [15360/16384 (94%)]\tLoss: 0.428496\n",
      "Test set: Average loss: 0.5024, Accuracy: 3211/4000 (80%)\n",
      "\n",
      "Train Epoch: 133 [0/16384 (0%)]\tLoss: 0.415053\n",
      "Train Epoch: 133 [5120/16384 (31%)]\tLoss: 0.416613\n",
      "Train Epoch: 133 [10240/16384 (62%)]\tLoss: 0.419255\n",
      "Train Epoch: 133 [15360/16384 (94%)]\tLoss: 0.405075\n",
      "Test set: Average loss: 0.5301, Accuracy: 3103/4000 (78%)\n",
      "\n",
      "Train Epoch: 134 [0/16384 (0%)]\tLoss: 0.479180\n",
      "Train Epoch: 134 [5120/16384 (31%)]\tLoss: 0.421788\n",
      "Train Epoch: 134 [10240/16384 (62%)]\tLoss: 0.372200\n",
      "Train Epoch: 134 [15360/16384 (94%)]\tLoss: 0.418324\n",
      "Test set: Average loss: 0.5946, Accuracy: 2822/4000 (71%)\n",
      "\n",
      "Train Epoch: 135 [0/16384 (0%)]\tLoss: 0.551668\n",
      "Train Epoch: 135 [5120/16384 (31%)]\tLoss: 0.439243\n",
      "Train Epoch: 135 [10240/16384 (62%)]\tLoss: 0.401086\n",
      "Train Epoch: 135 [15360/16384 (94%)]\tLoss: 0.398648\n",
      "Test set: Average loss: 0.5055, Accuracy: 3198/4000 (80%)\n",
      "\n",
      "Train Epoch: 136 [0/16384 (0%)]\tLoss: 0.411595\n",
      "Train Epoch: 136 [5120/16384 (31%)]\tLoss: 0.402175\n",
      "Train Epoch: 136 [10240/16384 (62%)]\tLoss: 0.399567\n",
      "Train Epoch: 136 [15360/16384 (94%)]\tLoss: 0.390113\n",
      "Test set: Average loss: 0.4960, Accuracy: 3251/4000 (81%)\n",
      "\n",
      "Train Epoch: 137 [0/16384 (0%)]\tLoss: 0.399189\n",
      "Train Epoch: 137 [5120/16384 (31%)]\tLoss: 0.396880\n",
      "Train Epoch: 137 [10240/16384 (62%)]\tLoss: 0.388050\n",
      "Train Epoch: 137 [15360/16384 (94%)]\tLoss: 0.454079\n",
      "Test set: Average loss: 0.5273, Accuracy: 3121/4000 (78%)\n",
      "\n",
      "Train Epoch: 138 [0/16384 (0%)]\tLoss: 0.481703\n",
      "Train Epoch: 138 [5120/16384 (31%)]\tLoss: 0.396191\n",
      "Train Epoch: 138 [10240/16384 (62%)]\tLoss: 0.434463\n",
      "Train Epoch: 138 [15360/16384 (94%)]\tLoss: 0.385456\n",
      "Test set: Average loss: 0.5578, Accuracy: 2979/4000 (74%)\n",
      "\n",
      "Train Epoch: 139 [0/16384 (0%)]\tLoss: 0.495377\n",
      "Train Epoch: 139 [5120/16384 (31%)]\tLoss: 0.384560\n",
      "Train Epoch: 139 [10240/16384 (62%)]\tLoss: 0.391112\n",
      "Train Epoch: 139 [15360/16384 (94%)]\tLoss: 0.425293\n",
      "Test set: Average loss: 0.5195, Accuracy: 3143/4000 (79%)\n",
      "\n",
      "Train Epoch: 140 [0/16384 (0%)]\tLoss: 0.442185\n",
      "Train Epoch: 140 [5120/16384 (31%)]\tLoss: 0.394592\n",
      "Train Epoch: 140 [10240/16384 (62%)]\tLoss: 0.389416\n",
      "Train Epoch: 140 [15360/16384 (94%)]\tLoss: 0.383763\n",
      "Test set: Average loss: 0.5295, Accuracy: 3098/4000 (77%)\n",
      "\n",
      "Train Epoch: 141 [0/16384 (0%)]\tLoss: 0.437159\n",
      "Train Epoch: 141 [5120/16384 (31%)]\tLoss: 0.413137\n",
      "Train Epoch: 141 [10240/16384 (62%)]\tLoss: 0.392325\n",
      "Train Epoch: 141 [15360/16384 (94%)]\tLoss: 0.408077\n",
      "Test set: Average loss: 0.5967, Accuracy: 2826/4000 (71%)\n",
      "\n",
      "Train Epoch: 142 [0/16384 (0%)]\tLoss: 0.555549\n",
      "Train Epoch: 142 [5120/16384 (31%)]\tLoss: 0.435284\n",
      "Train Epoch: 142 [10240/16384 (62%)]\tLoss: 0.375225\n",
      "Train Epoch: 142 [15360/16384 (94%)]\tLoss: 0.394633\n",
      "Test set: Average loss: 0.4893, Accuracy: 3262/4000 (82%)\n",
      "\n",
      "Train Epoch: 143 [0/16384 (0%)]\tLoss: 0.391102\n",
      "Train Epoch: 143 [5120/16384 (31%)]\tLoss: 0.382263\n",
      "Train Epoch: 143 [10240/16384 (62%)]\tLoss: 0.406533\n",
      "Train Epoch: 143 [15360/16384 (94%)]\tLoss: 0.410212\n",
      "Test set: Average loss: 0.5708, Accuracy: 2943/4000 (74%)\n",
      "\n",
      "Train Epoch: 144 [0/16384 (0%)]\tLoss: 0.546020\n",
      "Train Epoch: 144 [5120/16384 (31%)]\tLoss: 0.425626\n",
      "Train Epoch: 144 [10240/16384 (62%)]\tLoss: 0.415243\n",
      "Train Epoch: 144 [15360/16384 (94%)]\tLoss: 0.386778\n",
      "Test set: Average loss: 0.5059, Accuracy: 3216/4000 (80%)\n",
      "\n",
      "Train Epoch: 145 [0/16384 (0%)]\tLoss: 0.391045\n",
      "Train Epoch: 145 [5120/16384 (31%)]\tLoss: 0.387799\n",
      "Train Epoch: 145 [10240/16384 (62%)]\tLoss: 0.393923\n",
      "Train Epoch: 145 [15360/16384 (94%)]\tLoss: 0.364110\n",
      "Test set: Average loss: 0.5107, Accuracy: 3180/4000 (80%)\n",
      "\n",
      "Train Epoch: 146 [0/16384 (0%)]\tLoss: 0.450613\n",
      "Train Epoch: 146 [5120/16384 (31%)]\tLoss: 0.374841\n",
      "Train Epoch: 146 [10240/16384 (62%)]\tLoss: 0.382009\n",
      "Train Epoch: 146 [15360/16384 (94%)]\tLoss: 0.360939\n",
      "Test set: Average loss: 0.4893, Accuracy: 3271/4000 (82%)\n",
      "\n",
      "Train Epoch: 147 [0/16384 (0%)]\tLoss: 0.368997\n",
      "Train Epoch: 147 [5120/16384 (31%)]\tLoss: 0.375888\n",
      "Train Epoch: 147 [10240/16384 (62%)]\tLoss: 0.384468\n",
      "Train Epoch: 147 [15360/16384 (94%)]\tLoss: 0.410898\n",
      "Test set: Average loss: 0.5558, Accuracy: 3008/4000 (75%)\n",
      "\n",
      "Train Epoch: 148 [0/16384 (0%)]\tLoss: 0.512690\n",
      "Train Epoch: 148 [5120/16384 (31%)]\tLoss: 0.374543\n",
      "Train Epoch: 148 [10240/16384 (62%)]\tLoss: 0.398978\n",
      "Train Epoch: 148 [15360/16384 (94%)]\tLoss: 0.493314\n",
      "Test set: Average loss: 0.5620, Accuracy: 2974/4000 (74%)\n",
      "\n",
      "Train Epoch: 149 [0/16384 (0%)]\tLoss: 0.488271\n",
      "Train Epoch: 149 [5120/16384 (31%)]\tLoss: 0.384793\n",
      "Train Epoch: 149 [10240/16384 (62%)]\tLoss: 0.380905\n",
      "Train Epoch: 149 [15360/16384 (94%)]\tLoss: 0.468990\n",
      "Test set: Average loss: 0.5303, Accuracy: 3110/4000 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BasicNet().to(device)\n",
    "\n",
    "# 0.001 never learned\n",
    "# 0.1 too mutch?\n",
    "lr = 0.07\n",
    "optimizer = optim.SGD(model.parameters(), weight_decay=1e-5, lr=lr)\n",
    "\n",
    "#results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
    "#savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "for epoch in range(1, 150):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, valid_loader) # TODO: change for validation loader!!!!!!!!!!!!!\n",
    "    \n",
    "    if (acc > best_acc):\n",
    "        print(\"Best acc: \",acc)\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8185\n"
     ]
    }
   ],
   "source": [
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Game\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BasicNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (layer2): BasicBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): BasicBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (layer4): BasicBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer5): BasicBlock(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (sm): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "date = datetime.date.today().strftime(\"%B %d, %Y\")\n",
    "best_model.to(\"cpu\")\n",
    "torch.save(best_model, \"best_model_\" + date + \".pt\")\n",
    "torch.save(best_model.state_dict(), \"best_model_dict_\" + date + \".pt\")\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: loading a model\n",
    "Just un-comment the necessary lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Load entire model\n",
    "# loaded_model = torch.load(\"./best_model_January 30, 2019.pt\")\n",
    "# loaded_model.eval()\n",
    "# loaded_model.to(device)\n",
    "\n",
    "### Or, Load state_dict\n",
    "# model = BasicNet()\n",
    "# model.load_state_dict(\"./best_model_dict_January 30, 2019.pt\")\n",
    "# model.eval()\n",
    "# loaded_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate testing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Game\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "testImageFolder = \"./testset/\"\n",
    "\n",
    "test_official = DS.ImageFolder(root=testImageFolder, transform=tensor_img_transform)\n",
    "test_loader_off = data.DataLoader(\n",
    "    test_official,\n",
    "    batch_size=1,\n",
    "    # The sampler is an easy way to say that we're using the elements\n",
    "    num_workers=1,\n",
    "    pin_memory=True # Cuda\n",
    ")\n",
    "final_results = []\n",
    "ouput_file = open(\"finalResults.csv\", \"w\")\n",
    "# writre headers\n",
    "ouput_file.write(\"id,label\\n\")\n",
    "for idx, (img, label)  in enumerate(test_loader_off):\n",
    "    inputs = img.to(device)\n",
    "\n",
    "    output = best_model(inputs)\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]# [1] return s the arg max. i.e.  si label == target, get the index of the prediction\n",
    "    class_name_pred = full_dataset.classes[pred.int()]\n",
    "    # get the file name\n",
    "    path, filename = os.path.split(test_official.imgs[idx][0]) # (img path, label), no labels for tests, so we want arg[0]\n",
    "    filename, extension = filename.split(\".\")\n",
    "    # save the results\n",
    "    final_results.append([filename, class_name_pred])\n",
    "    ouput_file.write(str(filename) + \",\" + class_name_pred + \"\\n\")\n",
    "    \n",
    "\n",
    "ouput_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
